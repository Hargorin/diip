% ==============================================================================
%
%                             Scalability
%
% ==============================================================================
\chapter{Scalability} \label{chapt:scalability}
The code for this project is written from the beginning with the possibility to
scale it up in mind. The main idea being that the throughput can be increased.
Scalability in regard to this project can be divided into two terms:
\begin{itemize}
    \item Inside FPGA
    \item Across multiple FPGAs
\end{itemize}

In the following sections these two aspects are dissected theoretically. For
calculations the VHDL controller and image processing core is used with the
 metrics shown in table \ref{tab:sca:vhdlparams}.

\begin{table}[h!]
    \centering
    \begin{tabular}{l l p{10cm}}
        \toprule
        Parameter & Value & Description \\
        \midrule
        $b_w$  & $1 \frac{p}{clk} = 125 Mp/s$ & Input throughput of image
        processing core at 125MHz clock frequency in pixels per second. (1p
        $\equiv$ 1B) \\
        $b_e$  & $125MB/s$ & Ethernet throughput \\
        $w_l$  & $21$ & Window length of neighborhood \\
        \bottomrule
    \end{tabular}
    \caption{Parameters used for scalability proposal}
    \label{tab:sca:vhdlparams}
\end{table}

\section{Inside FPGA} \label{chapt:sca:inside}
Every FPGA has a given amount of ressources (LUTs, memory and so forth, as
described in table \ref{tab:XC7A200T}). The inside FPGA scalability aims towards
optimal usage of these ressources. Before methods for scalability can be
compared it must be clarified what can be scaled. The \gls{diip} project
consists of three main parts. The communication, controller and image processing
parts. The communication part is only implemented once to handle the
communication to the PC and therefore can not be scaled. The image processing
part however can be implemented multiple times inside the FPGA to increase
throughput. Therefore the conrtoller part splits up the incomming image data and
distributes it to the image processing cores. To summarize the inside FPGA
scalability:
\begin{itemize}
    \item Implement the communication part once
    \item Implement the image processing part as many times as the available
    ressources allow it
    \item Adapt the controller to distribute data accross the processing cores
\end{itemize}

Figure \ref{fig:insidefpgascaleconceptbd} clarifies the concept. Note that all
image processing cores are equal.

\begin{figure}[tb!]
    \centering
    \begin{adjustbox}{max height=5cm}
        \input{images/scalability/insidescalebd.tikz}
    \end{adjustbox}
    \caption{Block diagram of a inside FPGA scaled solution}
    \label{fig:insidefpgascaleconceptbd}
\end{figure}

Now that the concept is clarified, two possible solutions are compared to
distribute the image data accross the image processing cores, proposal A and B.
They differ in the way in what order the data is sent to the FPGA and how it is
cached by the controller inside the FPGA. For each proposal the following
metrics are calculated.

\begin{description}
    \item[Initial size $s_i$]\hfill \\
    The number of pixels that have to be sent to the FPGA before beginning
    interational operation.
    \item[Iteration size $s_r$]\hfill \\
    How many pixels that have to be sent to start a new iteration.
    \item[Store size $s_s$]\hfill \\
    How many pixels are cached inside the FPGA.
    \item[Number of inits per image $n_i$]\hfill \\
    Denotes the number of initial data transfers of size $s_i$ that have to be
    made per image.
    \item[Total tx size $s_{tx}$]\hfill \\
    The total number of pixels sent to the FPGA to calculate one image.
    \item[Number of image processing cores $N$]\hfill \\
    The number of image processing cores implemented on the FPGA.
\end{description}

Table \ref{tab:insidefpgascalability} explains two proposals with a simplified
window length of $w_l=3$ and two image processning cores ($N=2$). To understand
the procedure of the Wallis core, chapter \ref{ch:ip:concept} should be
consulted.

\begin{table}[b!]
    \centering
    \begin{tabular}{p{0.45\textwidth} p{0.45\textwidth}}
        \toprule
        \multicolumn{1}{c}{Proposal A} & \multicolumn{1}{c}{Proposal B} \\
        \midrule
        \multicolumn{1}{c}{
            \begin{adjustbox}{scale=0.7}
                \input{images/scalability/proposela.tikz}
            \end{adjustbox}}
        & 
        \multicolumn{1}{c}{
            \begin{adjustbox}{scale=0.7}
                \input{images/scalability/proposelb.tikz}
            \end{adjustbox}}
        \\
        % \multicolumn{1}{c}{caption}&
        % \multicolumn{1}{c}{caption}
        % \\
        \multicolumn{2}{c}{
            \begin{adjustbox}{max width=0.6\textwidth}
                \input{images/scalability/proposellegend.tikz}
            \end{adjustbox}
        }
        \\\midrule
        \textbf{Concept} & \\
        Image data is sent line wise. Each image processing core handles one
        input line, starts as soon as enough data is transfered and jumps $N$ lines
        downwards after line completion. Iinitialization transfer is done once
        per image. The image cache stores $w_l+(N-1)$ full image lines.
        &
        Image data is sent coloumn wise with $w_l+(N-1)$ pixels per coloumn. All
        image processing cores start processing at the same time and progress
        $N$ lines downwards after completion. An initialization transfer has to be done
        every $N$ lines. The image cache stores only the currently required
        pixels for the Wallis cores.
        \\\midrule
        \textbf{Initial size} & \\
        {\( 
            s_i = i_w(w_l+(N-1))
        \)}
        &
        {\( 
            s_i = w_l(w_l+(N-1))
        \)}
        \\\midrule
        \textbf{Iterarion size} & \\
        {\( 
            s_r  = i_w
        \)}
        &
        {\( 
            s_r  = w_l+(N-1)
        \)}
        \\\midrule
        \textbf{Store size} & \\
        {\( 
            s_s  = i_w(w_l+(N-1))
        \)}
        &
        {\( 
            s_s  = w_l(w_l+(N-1))
        \)}
        \\\midrule
        \textbf{Number of inits per image} & \\
        {\( 
            n_i  = 1
        \)}
        &
        {\( 
            n_i  = \frac{1}{N}(i_w-w_l+1)
        \)}
        \\\midrule
        \textbf{Total tx size} & \\
        {\( 
            s_{tx}  = i_w \cdot i_h
        \)}
        &
        {\( 
            s_{tx}  = n_i s_i + (i_w-w_l)s_r n_i
        \)}
        \\
        \bottomrule
    \end{tabular}
    \caption{Inside FPGA scalability proposals}
    \label{tab:insidefpgascalability}
\end{table}
\clearpage
Before comparing the two proposals another metric has to be taken into
consideration: The effective throughput of the image processing core. The way
the wallis filter works is that it requires a neighbourhood of pixels. For each
line the core processes, the initial neighbourhood has to be sent. Equation
\ref{eq:vhdlwallismaxb} derived in appendix 
\ref{app:derivations:theomaxvhdlwallis} is used to calculate the effective
throughput $b_r$ of a Wallis filter core. To simplify, the image height $i_h$ is
considered to be much larger than the window length $w_l$ and therefore the
equation can be noted as shown in equation \ref{eq:vhdlwallismaxb}.
Equation \ref{eq:vhdlwallismaxb} makes sense in that the Wallis filter core
requires $w_l$ pixels on the input to calculate one pixel on the output.
Therefore the output throughput (or real throughput) $b_r$ is by a factor of $w_l$
smaller than the input throughput $b_w$. Using the
throughput of the VHDL Wallis filter implementation ($b_w=125Mp/s$) and the
window length $w_l=21$ the real throughput $b_r$ is calculated:

\begin{align}
    b_r  & \approx \frac{b_w}{w_l} = 5.95 Mp/s
    \label{eq:vhdlwallismaxb}
\end{align}

Under the assumption that the real throughput $b_r$ scales proportionally with the
number of image processing cores implemented, equation 
\ref{eq:scaledrealttotalhroughput} can be noted to represent the total image
processing throughput $b_t$.

\begin{align}
    b_t  & = N \cdot b_r
    \label{eq:scaledrealttotalhroughput}
\end{align}

Considering the maximum throughput of the Gigabit-Ethernet connection of
$b_e=125MB/s$ \footnote{True Ethernet throughput is less than 125MB/s
considering packet overhead} equation \ref{eq:maxethernetthrouhgput} can be derived to calculate the
maximum number of image processing cores that can be implemented before the
Ethernet communication link is saturated.

\begin{align}
    b_e \geq b_t = N \cdot b_r \Rightarrow N \leq \frac{b_e}{b_r} \approx 
    \frac{b_e w_l}{b_w} = w_l = 21
    \label{eq:maxethernetthrouhgput}
\end{align}

To calculate the free memory the results from implementing the VHDL solution
\footnote{some of which are listed in table \ref{tab:ressource}} 
are dissected to calculate the available memory. Table \ref{tab:membudget}
lists the required block memory usage.

\begin{table}[h!]
    \centering
    \begin{tabular}{l r}
        \toprule
        Item & 36Kb BRAM tiles \footnote{One 36Kb BRAM tile can be used as two 18Kb RAMs} \\
        \midrule
        Total available & 365 \\
        TEMAC support & -2 \\
        UDP IP Core & -1.5 \\
        21 Wallis cores & -21 \\
        \midrule
        Free & 340.5\\
        \bottomrule
    \end{tabular}
    \caption{FPGA block memory budget}
    \label{tab:membudget}
\end{table}

After using up memory for the communication and image processing cores, 340.5
block RAM tiles with 36Kb each results in 1'496KB of remaining block RAM that
can be used for image caching inside the controller core to reduce the multiple
transmission of image data. Table \ref{tab:parsum} summarizes the
parameters that are used to compare the two inside FPGA scale proposals.

\begin{table}[h!]
    \centering
    \begin{tabular}{l r l}
        \toprule
        Parameter & Value & Description\\
        \midrule
        $N$ & 21 & Number of image processing cores to implement \\
        $s_b$ & 1'532KB & Block memory storage available for image caching \\
        $w_l$ & 21 & Window length of the Wallis filter \\
        \bottomrule
    \end{tabular}
    \caption{Parameter summary}
    \label{tab:parsum}
\end{table}

Now the two proposals can be put in contrast. Starting with figure 
\ref{fig:sca:compfixheight} the two proposals are compared against each other
with an input image of fixed height and variable with. This was chosen because
with the main limiting factor being \gls{bramtile} usage, the store size $s_s$ has to
be observed and it is for both proposals independant of image height $i_h$.
The image height $i_h$ is set to 10'000 arbitrary. The
two proposals can be differentiated in that proposal A is optimized for low tx
size $s_{tx}$ (and therefore optimized for throughput) while proposal B requires
buch less memory (area optimized). 

\begin{figure}[b!]
    \centering
    \begin{adjustbox}{max width=\linewidth}
        \input{images/scalability/compfixheight.tikz}
    \end{adjustbox}
    \caption{Comparing proposals width variable image width ($w_l=N=21$,
    $i_h=10'000$)}
    \label{fig:sca:compfixheight}
\end{figure}

\clearpage
The BRAM usage is best utilized by proposal A at approximately
$i_w=32Kp$\footnote{32 is chosen because it is a power of two} while
the necessary tx size is kept at the minimum. Its one disatvantage being the
limited image width of 32Kp. Three workarounds can be considered to raise the
limit in image width:
\begin{itemize}
    \item Use less image processing cores. This would decrease the required
    block memory and the image width can be increased. However by decreasing the
    number of image processing cores, the total throughput would also drop.
    \item Use proposal B. Its BRAM usage is significantly smaller however the tx
    size will increase.
    \item Split the image on the PC and process each coloumn as if it was a
    single image.
\end{itemize}

This problem leads to the next term in scalability: The scalability across
multiple FPGA.


\section{Across FPGA}
Now that one FPGA is capable of processing image data at the rate at which it is
trasfered to it through Ethernet, the solution can be scaled on a Ethernet
network. This
was the main reason to use Ethernet as communication basis. An input image is
split into parts on the computer and processed individually by FPGAs. The
processed pixels are merged together on the PC to form the output image. Figure
\ref{fig:sca:acrossfpgapart} illustrates a coloumn wise image splitting to three
parts. The colored boxes represent one Wallis filter processing a single line
with the filled boxed representing the processed output pixels.

\begin{figure}[b!]
    \centering
    \begin{adjustbox}{max width=\linewidth}
        \input{images/scalability/acrossfpgapart.tikz}
    \end{adjustbox}
    \caption{Image partitioning for distribution across multiple FPGA}
    \label{fig:sca:acrossfpgapart}
\end{figure}

For the Wallis filter operation, the split image data has to be overlapped
between the
segments to omit the loss of pixels where the image is cut. To calculate the
throughput of a distributed FPGA image processing solution, this multiple
sending of data has to be taken into consideration. Equation 
\ref{eq:sca:linesize} derives how the total line size $s_l$ is calculated in
reference to the number of FPGAs $N_F$ the image is distributed to. Width the
image width $i_w$ being much greater than the window length $w_l$, 
\ref{eq:sca:linesize} can be approximated.

\begin{align}
    s_l  & = i_w & (N_f = 1) \\
         & = i_w + (N_F-1) \cdot 2 \cdot \lfloor\frac{w_l}{2}\rfloor & \\
         & = i_w + (N_F-1)(w_l-1) & (w_l=2k+1, k \in \mathbb{N} )\\
         & \approx i_w & (i_w \gg w_l)
    \label{eq:sca:linesize}
\end{align}

Concluding this finding it can be infered that the throughput of a distributed
image processing solution scales proportionally to the number of FPGAs the image
is distributed to:
\begin{align}
    b_t & \propto N_F \\
        & \approx N_F \cdot b_e
\end{align}
\begin{tabular}{rl}
    $b_t        =$ & Total throughput of distributed image processing \\
    $N_F        =$ & Number of FPGAs used \\
    $b_e        =$ & Throughput of Ethernet connection to the FPGA \\
\end{tabular} \\

A last thought is put on the network infrastructure that is required to
process iamges distributed. The network link of 1Gb/s on the FPGA's Ethernet
port is saturated. Therefore if multiple FPGAs are used on a network, the
interconnecting network switch must be capable of an uplink to the PC or server
with a multiple of the throughput of the FPGA downlink. Figure 
\ref{fig:sca:network} illustrates a network where 10 FPGAs are connected to a
gigabit switch that features an uplink port supporting 10Gb/s throughput. An
example would be the GS418TPP switch \cite{netgearswitch}. 16 gigabit ports
could be used to connect to FPGAs and the SFP uplink port using fibre or copper
to connect to a PC.

\begin{figure}[b!]
    \centering
    \begin{adjustbox}{max height=5cm}
        \input{images/scalability/network.tikz}
    \end{adjustbox}
    \caption{Network topology required to distribute workload onto multiple
    FPGAa}
    \label{fig:sca:network}
\end{figure}

\section{Conclusion}
By scaling the Wallis filter operation inside the FPGA full advantage can be
taken of the available Ethernet bandwith. The controller core thereby caches as
many input lines as can be fitted into block memory to reduce multiple
transmissions of image data and increasing link efficiency. By dividing the input
image into multiple coloumns it can be distributed onto multiple FPGAs on a
network. This scalability across FPGAs is only limited by the network
infrastructure.
\\

% Assuming the VHDL implementation of the Wallis filter is used as image
% processing core, it can be scaled 21 times inside the FPGA before the Ethernet
% communication link is saturated. The image cache can be scaled to support
% an image width of 32Kp. Using a 10Gb/s switch and 10 FPGAs a total
% image processing throughput of $b_t=10\cdot1Gb/s=1.25GB/s$ can be achieved.
% \\

Taking a 1500Mp input image as specified in the technical requirements (see
appendix \ref{app:technicial_requirements}) and a 4:3 aspect ratio the image
width $i_w$ and height $i_h$ can be calculated. The resulting image width is
split into $N_F$ coloumns. Table \ref{tab:sca:exacrfpga} shows the total
processing time and throughput for one to four FPGAs. With only one FPGA used
the image is too wide. It is split into two coloumns and processed one after the
other on one FPGA.

\begin{table}[h!]
    \centering
    \begin{tabular}{l l l l l}
        \toprule
        $N_{FPGAs}$ & $N_{coloumns}$ & $i_w$ per FPGA &
        processing time & throughput \\
        \midrule
        1  & 2 &  22.4Kp & 12s & 62.5Mp/s \\
        2  & 2 &  22.4Kp & 6s  & 250Mp/s \\
        3  & 3 &  14.9Kp & 4s  & 375Mp/s \\
        4  & 4 &  11.2Kp & 3s  & 500Mp/s \\
        \bottomrule
    \end{tabular}
    \caption{Example scalability accross FPGA}
    \label{tab:sca:exacrfpga}
\end{table}

It can be concluded that a 1500Mp input image using four FPGAs on a network is
processed within three seconds resulting in a total throughput of 500Mp/s.

