Distributed FPGA for enhanced Image
Processing
Bachelor Thesis
By
N OAH H ÜTTER AND J AN S TOCKER

Degree Program
Coach
Expert
Team
Date

Electrical and Information Technology
Michael Pichler
Dr. Jürg M. Stettbacher
Noah Hütter, Jan Stocker
August 15, 2018

Content

© 2018

Noah Hütter
Jan Stocker

Design

Creative Commons

CC

University of Bristol Thesis Template [1]

BY:

Adapted by

Noah Hütter
Jan Stocker

This Bachelor Thesis was created in the spring semester of 2018 at the FHNW University of
Applied Sciences and Arts Northwestern Switzerland in the degree program Electrical and
Information Technology.

The code repository to this project is available at

https://gitlab.fhnw.ch/noah.huetter/diip

Last compiled on

Wed Aug 15 13:33:05 CEST 2018

Based on commit

b5522fe28ecd24e169fab85de65346513f051501

On host

malus.edu.ds.fhnw.ch

Version History
Version
Version
Version
Version

1.0.0
1.0.1
1.0.2
1.1.0

June 13, 2018

Initial Version

August 10, 2018

Version for proofreading

August 13, 2018

Proofreading completed

August 15, 2018

Print version

i

Abstract
In the world of self-driving cars and virtual reality games it is becoming increasingly important to represent digitally what we see. Therefore, using high resolution cameras, images
of the environment have been recorded. These large images are processed to be presented
three dimensionally. This image processing task needs to be accelerated to get a fast work
flow. A dedicated hardware approach using Field Programmable Gate Arrays (FPGA) was
implemented that is scalable onto multiple FPGAs. In a first approach High Level Synthesis
was used to describe a Wallis local contrast enhancement filter in C/C++ language that was
then synthesized to hardware description language. In order to further improve throughput a
VHDL solution was implemented. A memory management unit was introduced to cache necessary image data to reduce Ethernet bandwidth usage. The result is an image processing core
and a file transfer protocol stack on top of the User Datagram Protocol (UDP) to transfer the
images from a computer to the FPGA over gigabit Ethernet. The Wallis filter core processes
data at a rate of one pixel per clock at 125MHz which corresponds to 125 megapixels per
second. Studies on scalability show how the processing load can be distributed onto multiple
FPGA on a local area network and benchmarks present the performance against CPU based
image processing. With some additions, workload distribution is possible. This study proves
that a dedicated hardware approach for image processing is possible and will speed up the
process of creating virtual representations of reality.

Team members

Noah Hütter, Jan Stocker

Keywords

FPGA, UDP, Image Processing, Wallis Filter

iii

D EDICATION AND ACKNOWLEDGEMENTS

We wish to express our sincere thanks to those who supported us in completing this thesis. With
technical support from the guys at Nomoko, in particular Hakki Karaman and Luc Zeng, the
implementation of the Wallis filter became possible. We are grateful to Michael Pichler, lecturer
and Thesis coach, for providing us with his profound knowledge in FPGA development.
Further, we would like to place on record, our sincere thanks to the people who have made
themselves available for proofreading our work. In terms of spelling and reading flow, Tabea
Berger and Anita Gertiser provided us with their thoughts on the text. The feedback from Thomas
Hütter and Dino Zardet helped us phrase the findings and our line of thought in a comprehensible
manner.

v

A UTHOR ’ S DECLARATION

We declare that the work in this thesis was carried out in accordance with the requirements of the University’s Regulations and Code of Practice for Degree Programmes
and that it has not been submitted for any other academic award.
Except where indicated by specific reference in the text, the work is the candidate’s
own work. Work done in collaboration with, or with the assistance of others, is
indicated as such. Any views expressed in the thesis are those of the authors.

Noah Hütter

August 17, 2018

vii

Jan Stocker

C ONTENTS

1

Introduction

1

2

Theoretical Background

5

2.1

Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.1.1

Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.1.2

Wallis Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

Ethernet Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.1

Open Systems Interconnection (OSI) Model . . . . . . . . . . . . . . . . . . .

8

2.2.2

Physical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.2.3

Data Link . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.2.4

Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.5

Transport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

Vivado HLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

2.3.1

Design Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

2.3.2

Directives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

2.3.3

Arbitrary Precision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

AXI4 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

2.4.1

AXI4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

2.4.2

AXI4-Lite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

2.4.3

AXI4-Stream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

2.2

2.3

2.4

3

Mission

19

3.1

Starting Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

3.1.1

FPGA Board . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

3.1.2

Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

3.1.3

Image Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

3.1.4

Wallis Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

3.1.5

Development Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

Possible Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

3.2.1

Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

3.2.2

Dataflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

3.2

ix

3.2.3
3.3
4

7

Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23
25

4.1

Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

4.2

Implementation HLS and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .

27

4.2.1

Mean and Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

4.2.2

Division . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

4.2.3

AXI4-Stream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

4.2.4

Array Partition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

Implementation (VHDL) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

4.3.1

Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

4.3.2

Hurdles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

Dataflow

35

5.1

Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

5.2

Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

5.2.1

Acknowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

5.2.2

Retransmission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

5.2.3

Control Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

5.2.4

Data Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

5.2.5

Implemented Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

5.3.1

Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

5.3.2

Implementation HLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

5.3.3

Implementation VHDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

5.3

6

22

Image Processing

4.3

5

Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Scalability

55

6.1

Inside FPGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

6.2

Across FPGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

61

6.3

Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

Verification and Benchmark

65

7.1

Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

66

7.2

Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

7.2.1

Image Procession . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

7.2.2

Dataflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

69

7.2.3

Overall Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

71

Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

7.3.1

74

7.3

Compare Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
x

8

7.3.2

Compare Theoretical Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

7.3.3

FPGA against CPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77

7.3.4

Throughput against Area Efficiency . . . . . . . . . . . . . . . . . . . . . . .

78

Conclusion

79

8.1

Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

79

8.2

Dataflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

79

8.3

Complete System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

8.4

Working with High Level Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

8.5

Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

Glossary

81

Bibliography

83

List of Figures

85

List of Tables

86

A Appendices

89

A.1 Aufgabenstellung 2018 P6 Distributed FPGA . . . . . . . . . . . . . . . . . . . . . .

89

A.2 Technicial Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

94

A.3 Data Types of Wallis Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
A.4 UDP File Transfer (UFT) Protocol Specification . . . . . . . . . . . . . . . . . . . . . 103
A.5 UDP File Transfer Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
A.6 Images for Wallis Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
A.7 Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
A.7.1

Theoretical maximum Throughput of VHDL Solution . . . . . . . . . . . . . 111

A.7.2

Theoretical maximum Throughput of VHDL Wallis Core . . . . . . . . . . . 112

xi

CHAPTER

1

I NTRODUCTION

Self driving cars are getting more popular and virtual reality video games increasingly find their
way into people’s living rooms. One thing they share is the need for a digital copy of the world. Self
driving cars are trained in such worlds to accelerate the development of the algorithm and virtual
reality games are becoming more realistic. The Zurich based company Nomoko [2] is developing
a technology that will enable the creation of digital copies of the world. They built a giga pixel
camera and a 3D image processing pipeline to make this happen. This pipeline consists among
other things of high volume image processing. Modern computers and graphical processing units
(GPU) are fast in sequentially processing data but are designed to serve many different tasks
and not a specific one. A dedicated hardware approach designed for a specific image processing
task would expedite the 3D processing pipeline of creating digital copies of the world.
The goal of this project is to implement such an image processing task on a Field Programmable
Gate Array (FPGA). FPGAs consist of thousands of logical elements that can be configured and
connected together to form a complex logical operation. Together with on chip memory they
offer high throughput by processing the data in parallel in contrast to sequential. The data is
transferred to the FPGA over an Ethernet LAN connection for fast transfer rates. To accelerate
the computing even further, the system needs to be scalable to multiple FPGA boards to distribute
the workload.
In preparation for this thesis, a semester project focused on two building blocks for this work: The
communication block and the image processing block [3]. The results were two separate parts
working but neither connected nor optimized. This thesis builds upon the results of said work
and focuses on two main goals:

1

Chapter 1. Introduction

1. Increase throughput of the dataflow by improving the communication and by putting
thoughts on scalability
2. Implement a more complex image processing algorithm to show the real benefit of using
FPGA for image processing
The existing communication solution is file transfer oriented and stores received data in memory.
This has worked as a proof of concept but is not optimal for high throughput image processing.
The communication part is extended with an acknowledge mechanism that allows retransmission
of lost packets and a streaming interface which enables immediate processing of the image
data. Furthermore, a memory management unit is introduced that caches image data to reduce
bandwidth usage.
In the preceding project a Sobel filter was implemented using Vivado HLS. It uses a simple
convolution matrix for edge detection. This operation was well suited for the first image processing
task and to become acquainted with a high level synthesis tool. For this thesis a new image
processing task is realized: the local contrast enhancement operation called Wallis filter. The
algorithm is described and thoroughly tested in C/C++ and then synthesized to HDL by the Xilinx
Vivado HLS toolchain. To improve throughput and compare the results of high level synthesis
versus a low level HDL approach, the same filter is implemented in VHDL.
The main intention behind using Ethernet as communication was that the processing could be
distributed onto multiple FPGA. Methods on scalability are analyzed and compared.
An Artix7 Evaluation Kit by Xilinx serves as a development and testing platform. It is equipped
with gigabit Ethernet LAN and an FPGA with sufficient logic elements and memory for both the
communication and image processing task.
The result is a complete image processing pipeline that begins on a PC where the input image
is sent via Ethernet to the FPGA where it is processed and sent pack to the PC. The achieved
image throughput is 4.1MB/s primarily limited by the image processing core. The Wallis filter
core alone is capable of processing up to 125 megapixels per second on the input. A benchmark
shows the performance differences between a CPU based and FPGA based solution. Concepts on
scalability show how the processing power of FPGAs can be exploited if multiple FPGAs would
work on a network. The total throughput then increases proportional to the number of FPGAs
used in the cluster and multiple gigabytes per seconds of throughput is possible.
This report contains six main parts: Theoretical background, mission, image processing, dataflow,
scalability and verification/benchmark. The theoretical background explains the basics of image
processing, FPGA, Ethernet communication and on chip interfaces. The chapter mission will
cover the starting point and presents the concept. Chapters image processing and dataflow cover
the actual implementation process of the image processing and dataflow parts. Before verifying
these components in the verification and benchmark chapter the scalability is studied.
2

Project Report

3

CHAPTER

2

T HEORETICAL B ACKGROUND

This chapter provides the theory required for the project. It covers image processing and filtering
in general and the Wallis filter in detail, communication via Ethernet, the high level synthesis in
Vivaldo HLS and AXI4 interfaces.

2.1

Image Processing

In technical terms, image processing is the processing of image data. This includes image
processing, image analysis and the output of image files [4]. Procedures that generate a new
image can be distinguished into point operations, neighborhood operations and global operations
based on their input data. The next chapter compares these three operations while the following
chapter dissects the Wallis filter algorithm used in this project.

2.1.1

Operations

The types of operations that can be applied to images to transform an input image I(u, v) to an
output image I’(u, v) can be divided into three categories, which are explained in the following
three sections.
Point Operations
The point operations use the color or intensity information at a given pixel in the image as
input, calculates a new intensity value as the result and stores it to the same point in the target
image (figure 2.2a). Typical applications of point operations are, for example, the correction of
contrast and brightness, color correction by rotating the color space or the application of different
threshold value methods.
5

Chapter 2. Theoretical Background

Example for a point operation with u and v being pixel coordinates:
I 0 (u, v) = f (I(u, v))

(2.1)

Neighborhood Operations
Neighborhood operations use a certain amount of neighboring pixels as input (figure 2.2b). They
calculate and store the result at the reference point in the target image. Neighborhood operations
are often used in convolutional filters. These filters can be used, for instance, to implement
smoothing filters such as the Gauss filter. Convolutional filters can also be used to detect edges in
an image. For instance, this is possible with the Sobel filter.
Example to calculate the x-derivative and y-derivative with the Sobel matrix [5]:




−1 0 1
−1 −2 −1





(2.2)
Gy = I ∗
Gx = I ∗ 
0
0
−2 0 2
0

−1 0 1
1
2
1

(2.3)

Border Handling: With the application of neighborhood filters, the so-called border handling
problem occurs in every image because the filter protrudes beyond the original image (see figure
2.1). There are several possible solutions to this issue:
1. The border pixels are not considered. In the example image, the original image is two pixels
smaller in height and width than the original image as shown in figure 2.1.
2. If the filter exceeds the original image, the filter coefficients at the outside of the image are
set to zero.
3. The pixels required outside the image are extrapolated according to the closest pixels.
4. The image is continued periodically.

Figure 2.1: Border problem with neighborhood filter operations
6

2.1. Image Processing

(a) point operation

(b) neighborhood operation

(c) global operation

Figure 2.2: Image processing operations

Global Operations
Image analysis employs global image operations that uses the entire image as input data
(figure 2.2c). A typical application is finding regions or recognizing geometrical objects. Another
representative of this is the Hough transform and the Fourier transform.

2.1.2

Wallis Filter

The main goal of the Wallis filter is to adjust the mean and variance of an image and to map it
to given values. This is mainly used for local contrast enhancement in both low and high level
regions. It also achieves similar brightness levels in the image. The Wallis filter equation is [6]:
I 0 (x, y) =

(I(x, y) − µn ) · c · σ2g
c · σ2n + (1 − c) · σ2g

+ b · µ g + (1 − b) · µn

I(x, y) =

gray value pixel of the original image

0

calculated pixel with the Wallis algorithm

I (x, y) =
µn =

mean value of the neighborhood of the pixel I(x,y)

σ2n

variance value of the neighborhood of the pixel I(x,y)

=

µg =

given mean value

σ2g

given variance value

=

b=

brightness factor

c=

contrast factor
7

(2.4)

Chapter 2. Theoretical Background

(a) input image

(b) output image

Figure 2.3: Wallis filtered image

The two factors b and c can have the range from 0 to 1. The larger b is selected, the closer it
converges to the mean of the desired image µ g . The smaller b is selected, the more it converges
to the neighborhood mean µn . The bigger the factor c is chosen, the larger is the range of the
variance constant σ2g [6].
The size of the neighborhood depends on the sensor resolution of the camera. The size can be
selected from a minimum of 11 to a maximum of 41 [7]. Through the neighborhood operation the
smoothing operator is introduced. This means it reduces noise and improves the signal to noise
ratio of the image. This in turn enhances image quality [6].
Figure 2.3 shows an image before and after filtering by the Wallis filter. It can be seen that in
image (b) more structural detail is visible thanks to the local contrast enhancement without
overexposing bright areas.

2.2

Ethernet Communication

Exchanging data between two devices can be done using different approaches. The following
section contains an overview of the communication standards used in local area networks (LAN).
This section is taken from the preceding project report [3] to clarify the communication concept.

2.2.1

Open Systems Interconnection (OSI) Model

Such a telecommunication system can be characterized by the Open Systems Interconnection
Model. The OSI model is a stack of seven abstraction layers grouped into two groups: The host
layers and the media layers (see figure 2.4). Each layer serves the layer above it and is served
with data from the layer beneath it. In the following chapters the OSI reference model is used to
characterize the Ethernet standard.
8

2.2. Ethernet Communication

Application
Network Process to Application

Data

L7

Data

Presentation
Data representation and Encryption

L6

Data

Session
Interhost communication

L5

Host
Layers

Transport
Segments

Packets

Media
Layers

Frames

End-to-End connections and Reliability
Network
Path Determination and IP (Logical
addressing)
Data Link
MAC and LLC (Physical addressing)

L4

L3

L2

Physical
Bits

Media, Signal and Binary Transmission

L1

Figure 2.4: OSI Model

2.2.2

Physical

The first layer of the OSI model is the physical layer. It defines the electrical and physical
specification of the connection. In the case of local area networks the connection medium is
usually copper. The circuitry required to implement the physical layer is done by the PHY-Chip.
This integrated circuit provides digital access through a media independent interface (MII) to the
analog physical data link.

2.2.3

Data Link

The data served from the physical layer is then passed to the data link layer. Its main purpose
is to ensure a reliable transfer of data frames between two nodes connected by a physical layer.
It may also provide means to detect errors that may occur in the physical layer. Ethernet is the
protocol used in the data layer of local area networks and the layer is split into two sublayers,
the logical link control (LLC) and the medium access control (MAC). The LLC provides means to
allow multiple network protocols (OSI layer three) to be multiplexed onto the same medium. The
MAC encapsulates higher level frames into frames appropriate to be transmitted by the physical
layer.
Figure 2.5 shows an Ethernet frame. The first seven bytes consist of a fixed preamble. It allows
devices on the network to easily synchronize their clocks for bit-level synchronization. It is
followed by the start frame delimiter (SFD) that marks the beginning of a frame. Sender and
receiver MAC addresses ensure that the packet is received by the corresponding host and that
9

Chapter 2. Theoretical Background

Basic MAC Frame

Preamble SFD Destination MAC Adr. Source MAC Adr.

Type
Data (46 - 1500 Bytes) PAD CRC
Field

Figure 2.5: Ethernet Frame

it can reply to the sender. The type field indicates the protocol used on the next layer (network
layer). After the data payload a frame check sequence in form of a CRC (cyclic redundancy check)
is sent to provide error detection. The maximum data payload size is limited to 1500 bytes.
The medium access controller is implemented in hardware to ensure that every bit is received
and stored. The transmit and receive data is commonly stored in FiFo (first in first out) buffers.
This way the next layer in the OSI model is not required to have low latency capability.

2.2.4

Network

The data link layer provides means to send frames across nodes in the same network. As soon
as the destination node is in another network, a network layer is required. Using logical device
addresses, network packets can be routed across different networks and on different media. This
allows data to be sent over long distances.
The most commonly used network layer is the Internet Protocol version 4 (IPv4). It consists of a
20 byte sized header that contains destination and source IP addresses, total length, checksum
and other fields. The protocol field indicates what layer four protocol is used. Figure 2.6 shows a
complete IPv4 header.

Figure 2.6: IPv4 header [8]
10

2.3. Vivado HLS

2.2.5

Transport

The transport layer is the first layer in the OSI model that is not required by the network. Its
main purpose is to control the communication of different applications on two hosts. Therefore
a port number is required to distinguish between the different applications utilizing the same
network connection. The transport layer may also provide segmentation of the data, guarantee of
delivery and flow control to avoid network jam.
The two most used transport layer protocols are the Transmission Control Protocol (TCP) and
the User Datagram Protocol (UDP). Table 2.1 shows the main differences between these two
protocols. The most important aspect is the protocol connection setup. While UDP is connection
less (data is sent without setup), TCP establishes a connection between host and client prior to
data transmission. This ensures a reliable data delivery hence all messages are acknowledged.
UDP has its benefits in lower overhead and for that reason has slightly higher transmission
speed but the protocol does not guarantee that the message has been received by the client.

Connection oriented
Header size
Reliable transmission
Acknowledge
Segmentation
Best for

TCP

UDP

Yes
20 Byte
Yes
Yes
Yes
reliable transfer

No
8 Byte
No
No
No
fast transfer

Table 2.1: TCP vs. UDP

2.3

Vivado HLS

Vivado HLS is a development environment that synthesizes C/C++ code to HDL. The C/C++ language is intended for sequential programming where the HDL language describes the hardware.
This chapter deals with the design flow ( 2.3.1) of Vivado HLS. The directives (2.3.2) that are used
for the compiler to synthesize the C/C++ code in HDL and finally arbritrary data types (2.3.3)
which can be used in Vivado HLS for C/C++ code.

2.3.1

Design Flow

In Vivado HLS it is possible to translate the C/C++ code into HDL and generate an IP core for
Vivado HLx. To ensure that the algorithm works correct, it can be validated in various steps.
These steps are the design flow of Vivado HLS and are shown in figure 2.7. First of all, the
C/C++ algorithm can be tested in a simulation. This requires a testbench in addition to the C/C++
algorithm. As soon as the algorithm works satisfyingly the design can be run through synthesis
11

Chapter 2. Theoretical Background

C/C++ Algorithm

C/C++ Testbench
C/C++ Simulation
C/C++ Synthesis
VHDL
RTL Simulation
Package IP
Vivado HLx

Figure 2.7: Vivado HLS design flow

and an RTL design can be generated. HDL design can be verified in an RTL simulation. The
results of the RTL simulation must match the results of the C/C++ simulation. If this is the case,
an IP core can be generated from the HDL code and used in the Vivado HLx [9].

2.3.2

Directives

If the FPGA is configured using C/C++ language, there is no way to get around directives.
Directives are instructions to the compiler on how it should synthesize and optimize the code.
Latency, throughput or area can be optimized [10]. The following three chapters explain some
important directives to reduce the latency and increase the throughput.
Directives are also called pragmas and can be written directly into the C/C++ code or added as
directives in a separate tcl-file.
Interface
The directive interface specifies the input and output ports of the IP core. Different interfaces
can be implemented in the IP core as input and output ports. Among others, these are control
interfaces or interfaces for data. The IP core can be controlled with the control interfaces. With
this signal it can be started or stopped for example or send a handshake if the IP core has finished
its operation. AXI4 interfaces can also be added directly to the IP core using directives. The
AX4-Lite, AXI4-Master or AXI4-Stream interface can be configured. Among other things, memory
elements can also be connected directly to the IP core. These are block RAM or FiFo that can be
used to read and process the data inside the IP core. These can also be used to store output data
of the IP core [10].

12

2.3. Vivado HLS

Figure 2.8: Pragma: interface

Figure 2.8 shows how the pragma interface is configured. First, the interface can be specified
and whether it is an input or an output. Second, the variable to which the pragma is to be applied
has to be specified.
Array Partition
The array_partition directive splits an array into several smaller elements. This means that
in RTL synthesis several smaller memories or registers are used to form a larger memory. This
increases the throughput in the design but also the read and write ports for the storage.
Figure 2.9 shows the three different methods of partitioning an array.
Block
The whole array is split into several arrays of the same size. The elements are stored in the
same order as in the array. The array is simply cropped apart.
Cyclic
The array is split into equal parts like block, but the storage order is different. As can be
seen, each following element is stored in a different memory. This enables concurrent read
of multiple successive elements.
Complete
This argument splits the complete array into individual elements.

Pipeline / Unroll
In Vivado HLS the FPGA is described with C/C++ language. Among others, it is also possible to
program loops. These are still processed sequentially by the C/C++ simulation, as can be seen in
figure 2.10 (sequential) with nine clock cycles. The loop includes read and write access, as well as
13

Chapter 2. Theoretical Background

Figure 2.9: Pragma: array_partition

data processing. This loop is executed three times. The directives can improve the performance of
the loop. Two directives are available to increase the throughput of a loop. These are pipeline
and unroll.
Pipeline
This directive allows a loop to be pipelined which increases throughput. Now more operations are happening in a single clock cycle. Pipeline is used if not all data for processing is
available in the loop. Figure 2.10 (pipeline) shows that only one read access per clock cycle
can be made. The other operations can be pipelined. This will lead to a processing of the
loop in five clock cycles. The throughput could therefore be increased from the sequential
method with one third data per clock cycle to pipelined with one data per clock cycle.
Unroll
This directive allows the entire loop to be parallelized. This is only possible if all data is
available at the beginning of the loop. Figure 2.10 shows that the processing of the loop
only requires three clock cycles and the throughput could be increased in comparison to
pipeline to three data per clock cycle.
With nested loops, note that if pipeline is applied to the outer loop, the inner loops are automatically unrolled.
One can also apply both directives in combination in a loop. This is useful if, for example, the
data was previously split per array_partition and it is now possible to read multiple data (but
not all data) in one clock cycle. Since the unroll directive can specify a factor that unrolls the loop.

14

2.3. Vivado HLS

Figure 2.10: Pragma: pipeline / unroll

2.3.3

Arbitrary Precision

Resources are limited on the FPGA. Therefore it is often a big waste of resources when calculating
with too large data types or even floating point on the FPGA [11]. Viviado HLS supports arbitrary
precision data types using C/C++ code. With this it is possible to counteract the problem of large
data types and floating point.

Arbitrary Precision Integer: Arbitrary Precision Integers are data types which whom it is
possible to define any bit width. This has the advantage that resources can be reduced. Because
it is not necessary to select the data types provided in C/C++ (8, 16, 32 bit or char, short, int etc.)
To use these data types only the header file ap_int.h must be included. The following example
shows how an 11 bit signed integer and a 5 bit unsigned integer are defined [12].
1

ap_int <11> var1 ;

2

ap_uint <5> var2 ;

Arbitrary Precision Fixed-Point: Floating Point can be used on the FPGA, but this usually
requires a lot of resources. Viviado HLS has a library for Arbitrary Precision Fixed Point data
types, which can be used to calculate sufficiently accurate. With these fixed point data types
many resources can be saved.
For this purpose there is the library ap_fixed.h and has to be included. The data types are
defined so that the first number represents the whole data type width and the second number
represents the above number of the binary point. The following example shows an unsigned 11
bit variable with 3 bits representing the integer value and 8 bits representing the fractional value
[12].
1

ap_ufixed <11,3> fp_var ;

15

Chapter 2. Theoretical Background

Transaction Channel

Use

Write address channel
Write data channel
Write response channel
Read address channel
Read data channel

Generate write requests to slave
Write data to be sent to the slave
Write response from slave to master
Generate read requests to slave
Read data from slave to master

Table 2.2: AXI4 transaction channels

2.4

AXI4 Interfaces

Advanced eXtensible Interface or AXI is part of the Advanced Microcontroller Bus Architecture
(AMBA®) introduced by ARM®[13]. It is intended for the interconnection of functional blocks
inside a System on Chip or FPGA. IP cores offered by Xilinx implement these interfaces. Two types
of AXI interfaces are destinguished: Memory mapped and stream. Memory mapped interfaces
intend the bidirectional access to data stored in registers while streaming interfaces offer a
unidirectional dataflow [14].

2.4.1

AXI4

The AXI4 interface is a memory mapped interface and consists of five transaction channels as
shown in table 2.2.
Each channel has its own handshake signals. Handshake signals are used to indicate that the
sender has valid data and the receiver is ready to accept data. Figure 2.11 shows the handshake
signals. The VALID signal is set active if the sender has driven the INFORMATION lines with
valid data. The receiver indicates with a READY impulse that it has read the information.
An AXI4 write transaction could run as followed:
1. Master sends address and burst size through write address channel
2. Master sends data through write data channel
3. Slave responds with status on write response channel

Figure 2.11: VALID before READY handshake [13]
16

2.4. AXI4 Interfaces

An AXI4 read transaction could run as followed:
1. Master sends read address and burst size through read address channel
2. Slave responds with data on read data channel
A burst transaction allows to send or receive a range of data with a single write request while a
single beat transaction consists of one write request and one data word.

2.4.2

AXI4-Lite

AXI4-Lite is a scaled-down version of AXI4. It allows single beat write and reads but no burst
transaction. It therefore requires less signals and is suited for configuring IP cores and register
values. It consists of the same transaction channels and handshake signals. AXI4 and AXI4-Lite
are compatible and can be converted.

2.4.3

AXI4-Stream

AXI4-Stream is a stream based interface. Data is sent in one direction from master to slave. It is
not address based and features only one transaction channel. There is also no burst size limit.
There are four required signals as shown in table 2.3.

TVALID and TREADY are the handshake signals as described in chapter 2.4.1 and illustrated in
figure 2.11. AXI4-Stream defines further sideband signals, some of which are described in table
2.4. In this project the TLAST signal is used frequently to indicate for example the end of a file
transfer of the communication core or the last pixel of an image line.

Signal

Source

Description

ACLK
ARESETn
TVALID
TREADY
TDATA[(8n-1):0]

Master
Master
Master
Slave
Master

Rising edge sampled clock signal
Global active low reset signal
indicates that the master is driving a valid transfer
indicates that the slave can accept a transfer
Data payload

Table 2.3: AXI4-Stream required signals
Signal

Source

Description

TLAST
TDEST[(d-1):0]
TUSER[(u-1):0]

Master
Master
Master

indicates the boundary of a packet
provides routing information for the data stream
user defined sideband information

Table 2.4: AXI4-Stream sideband signals
17

CHAPTER

3
M ISSION

With the theoretical background covered the mission of this project can be addressed. The following sections will first cover the starting point and the available resources. Different possible
solutions will be covered in 3.2 before presenting the concept in chapter 3.3.
In a world of self driving cars and virtual reality, having a digital copy of the real world yields
several benefits. Cars can be trained in a virtual city to increase the performance of their
algorithms and video games could get more realistic if the player could walk through the streets
of a major city. Gathering this data is one problem and processing the images is another. Pictures
have to be converted, analyzed and processed. This requires a great deal of computational power
for a task that is repeated several times.

3.1

Starting Point

The goal of this project is to investigate a dedicated hardware approach to accelerate intense
image processing. To do so, a basic image processing algorithm such as the Wallis filter is
implemented on an FPGA that communicates over a network with a host computer. The image
processing task should later be distributed onto multiple FPGAs to accelerate the processing
even more [15]. Before listing possible solutions the available resources and boundary conditions
need to be analyzed.

3.1.1

FPGA Board

The Xilinx AC701 Evaluation Kit is used as development platform. It features an Artix-7 FPGA
and several useful on-board peripherals. The JTAG interface is used for configuration and
19

Chapter 3. Mission

Figure 3.1: Xilinx AC701 Evaluation Kit [16]

debugging of the FPGA. To connect the platform to the network, a Gigabit Ethernet PHY handles
the first layer of the OSI model in hardware (see chapter 2.2.2). Storing data is possible in the
DDR3 memory module. Table 3.1 shows a summary of the important peripherals on the AC701
board.
The XC7A200T FPGA is part of the Artix-7 family. It is optimized for high logic throughput at low
costs. Important for this project is the number of logic cells and the amount of on-chip memory.
With more logic cells available, data can be processed parallel and the throughput increases.
Processing more data at the same time also requires the data to be stored. Therefore on-chip
memory, also called block memory, is used to have fast access to the data. Table 3.2 lists the
relevant numbers of resources.

Part

Description

FPGA
JTAG
Memory
Ethernet

XC7A200T-2FBG676C
Onboard JTAG configuration circuitry to enable configuration over USB
DDR3 SODIMM 1GB up to 533MHz / 1066Mbps
10/100/1000 Mbps Ethernet (RGMII)
Table 3.1: Xilinx AC701 key board features [17]
20

3.1. Starting Point

XC7A200T
Logic Cells
DSP Slices
Block Memory

215k
740
1642 KB

Table 3.2: XC7A200T key features [17]

3.1.2

Communication

In the preceding project [3] an Ethernet communication between PC and FPGA was established.
This included a UDP based protocol that was implemented on the PC and on the FPGA in form of
an IP core. Basic file transfers are working but many features such as reliable transfer using
acknowledge and retransmission only exist on paper. The work in this project is built upon the IP
core from project 5 while improvements and modifications are made to the IP core.

3.1.3

Image Size

The customer Nomoko has a camera with a resolution of 1500MPixel (A.2). If this is a color image
with 24 bit RGB values, the following file size will result:
F il eSize = P ixel tot · P ixel size = 1500MP ixel · 24bit = 36Gbit ∼
= 4.4GB yte

3.1.4

(3.1)

Wallis Filter

The customer Nomoko wants the Wallis filter to be implemented. The Wallis filter is used for
local contrast enhancement. For example, the filter can compensate house shadows in images
and get more details from the image. The Wallis filter is based on the equation 2.4 as explained
in chapter 2.1.2.

3.1.5

Development Environment

The image processing algorithm is written in Vivado HLS (A.2). With Vivado HLS, an IP core for
the Vivado HLx can be generated from a C/C++ code. Vivado HLS is useful when programming
algorithms and afterwards translating them into hardware description language.
Communication is implemented with configurable IP blocks provided by Xilinx, third party
providers and custom developed IP cores. Vivado HLS is mainly used for the dataflow inside the
FPGA (see the technical requirements for more detail in appendix A.2).
21

Chapter 3. Mission

3.2

Possible Solutions

With the starting point dissected the possible solutions to the problem can be stated. They are
split into the two parts of image processing and dataflow.

3.2.1

Image Processing

The Wallis algorithm is based on the neighborhood operation (2.1.2). Therefore, the borders of
the image must be considered separately (2.1.1). There are three possible solutions to the border
problem:
1. The border pixels are not considered. This means that the destination image will be smaller
depending on the size of the neighborhood
2. The pixels required outside the image are extrapolated according to the closest pixels
3. The image is continued periodically

3.2.2

Dataflow

As described in 2.1.2 the Wallis filter requires a neighborhood of pixels to do its calculations.
Therefore the data comming from the communication part must be buffered and fed to the Wallis
filter in a given order. Further details are described in chapter 5. Following realizations may be
considered for the control of the dataflow:
Data preparation on PC: The image data is prepared on the PC and sent in the right order.
This method is simple to realize and requires no additional FPGA resources. On the other hand,
data may be sent more than once which results in a decrease of throughput.
Soft microprocessor core: Image data is sent to the FPGA and stored in FPGA memory. A
microcontroller implemented in FPGA logic then dissects the data and feeds it to the Wallis filter
in the right order. Hence the data is buffered the Ethernet throughput can reach its maximum.
The downside to this solution is that such a soft microprocessor core may take more FPGA
resources than a dedicated approach.
Controller IP core: An IP core is developed that handles the specific task of managing the
dataflow between the communication and Wallis cores. This approach takes little FPGA resources
but involves more time to develop.

3.2.3

Scalability

As the thesis title suggests the image processing is distributed, meaning that the data to be
processed is split among multiple image processing cores. This can be done inside an FPGA and
22

3.3. Concept

across multiple FPGAs. Chapter 6.6 in appendix A.2 suggests two possible solutions to scale the
image processing inside the FPGA.
1. Store the image in block memory and distribute it to the image processing’s local memory
for processing
2. Data is streamed from the communication part directly to the different image processing
cores
Scalability across multiple FPGA is analyzed in theory.

3.3

Concept

Now that the possible solutions for dataflow, image processing and scalability have been discussed,
the way of proceeding can be concluded.
Image processing
1. The border pixels are not considered for simplicity. The destination image will be smaller
than the source image.
2. The image is sent in several pieces to the FPGA since 4.4GB (see equation 3.1) do not have
space on the FPGA (see chapter 3.1.1).
Dataflow
1. The communication is extended with acknowledge and retransmission.
2. A controller IP core is realized to handle the dataflow between communication and Wallis
filter.
Scalability
1. Use stream interfaces on the image processing and dataflow parts to enable stream based
scalability
2. Assess the inside and across FPGA scalability in theory
Figure 3.2 shows the conceptual block diagram. Image data is sent from the PC to the FPGA over
Ethernet. The communication block handles the file transfer and stores the image data in FPGA
block memory. A controller core then feeds the data in the right order to the Wallis filter and
stores the results again in block memory. The controller sends a command to the communication
cores which then starts sending the processed image data back to the PC.

23

Chapter 3. Mission

LAN

FPGA
1.

2.
Memory

Communication
8.

7.
6.
Image
Processing

Figure 3.2: Block diagram

24

3.

5.
Controller
4.

CHAPTER

4

I MAGE P ROCESSING

The Wallis as a local contrast enhancement filter is implemented as an IP core. First the concept
of the algorithm is shown and then the implementation of the filter in Vivado HLS using C/C++
code is documented. The simplifications and optimizations of the algorithm with Vivado HLS are
explained in chapter 4.2.
During the work on the project it has been discovered that the throughput does not reach the
maximum and therefore will not lead to the optimum solution (see chapter 7). This is why a
second implementation of the Wallis filter was made using VHDL. This second implementation is
explained in chapter 4.3 with a concept of the VHDL design.

4.1

Concept

Figure 4.1 shows the concept of the C/C++ code programmed in Vivado HLS. First, the local mean
and variance values must be calculated so that the Wallis filter can be applied with the given
parameters to the pixel I(x, y). The calculation of mean and variance is explained in chapter 4.2.1.
The calculated pixel is I 0 (x, y).
The sequence of the code is shown in figure 4.2. It consists of an initialization and an iteration.
During the initialization, the complete neighborhood (here as an example 3x3) is read and the
central pixel of the neighborhood is calculated. To calculate the next pixel, only one new column
(gray marked) is read and the old column (hatched line) can be dismissed. This step is the so
called iteration.
The operation is row based. This means that each new output row of the image requires the
initialization procedure.
25

Chapter 4. Image Processing

Mean & Variance

Σ

Wallis Filter
µn

1
N

σ2n

pixel

I(x, y)

()2

Σ

()2

1
N

I 0 (x, y)

I 0 (x, y)

parameters

−

Figure 4.1: Concept of the Wallis filter implementation
Initialization

Iteration

Iteration

Initialization

Iteration

Iteration

new pixels

dismissed pixels

Figure 4.2: Sequence of the implementation with initialization and iteration

Abbreviations
Table 4.1 lists constants which were used in the code and recur in the documentation. WIN_SIZE
is the number of pixels inside the Wallis filter neighborhood. This neighborhood is square
therefore the side length WIN_LENGTH equals to the square root of WIN_SIZE.

Name

Value

Description

WIN_SIZE
WIN_LENGTH

441
21

Chosen as neighobrhood (N)
p
W I N_SI ZE

Table 4.1: Constants of the code

26

4.2. Implementation HLS and Optimization

4.2

Implementation HLS and Optimization

The following three chapters explain the implementation of the Wallis filter in C/C++ code. The
first chapter covers the simplification of the mean and the variance calculation. Then the fixed
point calculation of the Wallis equation and the optimizations using pragmas in Viviado HLS are
explained.

4.2.1

Mean and Variance

In this section the calculation of the mean and the variance values is investigated, so that it can
be implemented more efficiently on the FPGA. The problem is that the equation known for the
variance (4.2) is dependant of the mean value (equation 4.1).
No adjustment is necessary for the equation for the mean value. It can be used as is. Equation
4.1 is used to calculate the mean value:

µ=

−1
1 NX
xi
N i=0

(4.1)

The equation known for variance (4.2) is adjusted before implementated on FPGA. As can be seen,
the mean value must first be calculated so that the calculation of the variance can be started. The
goal is to start the calculation of the mean and variance values at the same time so that it can be
implemented more efficiently on the FPGA. Equation 4.2 is expanded to give equation 4.3. If it is
represented a little different equation 4.4 can be noted. In this case it is to be recognized that in
P N −1
1 P N −1
the second term the mean value results µ = N
i =0 x i . In the third term, the sum
i =0 1 equals
to N. This transformation leads to equation 4.6 implemented on the FPGA. The implementation
of the equation is shown in the concept in figure 4.1 in the mean and variance block. In other
words, the mean and variance values can now be calculated simultaneously.

σ2 =

−1
1 NX
(x i − u)2
N i=0

−1
1 NX
(x2 − 2µ x i + µ2 )
N i=0 i
P N −1
−1
−1
x i 1 2 NX
1 NX
2
x i − 2µ i=0
+ µ
1
=
N i=0
N
N
i =0
P N −1 2
i =0 x i
=
− 2µ 2 + µ 2
N
P N −1 2
i =0 x i
=
− µ2
N

=

27

(4.2)
(4.3)
(4.4)
(4.5)
(4.6)

Chapter 4. Image Processing

Listing 4.1 shows the summing of pixels. The pixels are summed for the mean and squared
summed for the variance at the same time. Furthermore, the pixels are temporarily stored in an
array so that they can be used again at a later time.
Because this is the initialization, the entire neighborhood (WIN_SIZE) is read. During the
iteration sequence only one pixel column (WIN_LENGTH)) is read.
1

loo p_r data : f o r ( uint16_t i = 0 ; i < WIN_SIZE ; i ++) {

2

i n P i x e l = inData . read ( ) ;

3

p i x e l [ i ] = i n P i x e l . data ;

4
5

sum_Pixel += p i x e l [ i ] ;

6

tmp_pow = p i x e l [ i ] * p i x e l [ i ] ;
sum_Pixel2 += tmp_pow ;
/ / sum o f the p i x e l s ^2

7
8

/ / sum o f the p i x e l s

}

Listing 4.1: Calculation of the sum
In a function the sum of pixels are passed to and the mean and variance values are calculated.
1

mean = ( sum_Pixel / WIN_SIZE ) ;

2

var = sum_Pixel2 / WIN_SIZE − ( mean * mean ) ;

Listing 4.2: Calculation of mean and variance values

4.2.2

Division

This section explains how the division of equation 2.4 for the Wallis filter is implemented. It is
known that divisions on the FPGA take a long time and also require a lot of resources. Therefore
the division will be examined in detail to optimize the code for throughput.
First, the accuracy of the division is commented because resources can already be saved by
limiting accuracy. As discussed in chapter 2.3.3, a lot of resources can be saved with the accuracy
and when ap_ufixed is used. Finally, the calculated Wallis pixel is rounded to an integer between
0 - 255. This means that the intermediate results do not have to be too accurate. With Matlab and
the simulation of the C/C++ code in Viviado HLS, this hypothesis was tested and the accuracy
of the intermediate variables was adjusted. The data types ap_[u]int and ap_[u]fixed are used
instead of floating point. The size of the data types can be found in table 4.2. Some of the data
sizes do not match the min/max value. The reason is that the results are rounded and an overflow
may be based on the next larger data size. For security reasons, the data types are usually one
bit larger. A table with a description of the variables can be found in the appendix A.3.

28

4.2. Implementation HLS and Optimization

Name

Type

Size

min Value

max Value

iPxl
g_Mean
g_Var
n_Mean
n_Var
brightness
contrast
b_gMean
ci_gVar
tmp_Num
num
c_nVar
bi_nMean
den_Var
rec
den
div
w_pixel

ap_uint
ap_uint
ap_uint
ap_uint
ap_uint
ap_ufixed
ap_ufixed
ap_uint
ap_uint
ap_int
ap_int
ap_uint
ap_uint
ap_uint
ap_ufixed
ap_ufixed
ap_int
ap_int

8
8
14
8
14
5,1
5,1
8
14
23
23
14
8
14
18,1
18,1
23
23

0
0
0
0
0
0
0
0
0
-4177665
-4177665
0
0
0
1
≈0
-4177665
-4177665

255
255
16383
255
16383
1
1
255
16383
4177665
4177665
16383
255
16383
1
1
4177665
4177665

Dependence

brightness, g_Mean
contrast, g_Var
iPxl, n_Mean, g_Var
tmp_Num, contrast
n_Var, contrast
n_Mean, brightness
n_Var, ci_gVar
rec, den_Var
num, den
div, b_gMean, bi_nMean

Table 4.2: Accuracy of data types in the Wallis filter division

Division
x
y
1
y

Latency [clocks]
28
23

·x

Table 4.3: Comparision of division methods

Now a closer look at the calculation of the division is taken. The equation of the Wallis filter
results in the division of

x
y.

This means that the dividend and the divisor change with each

pixel. This could be programmed on the FPGA with
on the FPGA is faster than a division. Therefore the

x
y . But as it is known, a multiplication
x
y division is replaced with a reciprocal

version 1y , where afterwards the dividend is multiplied by the result of the reciprocal division
( 1y ) · x. The latency of these two methods were measured in Vivado HLS and are shown in Table 4.3.

The reciprocal version was chosen because it has a lower latency. In addition, the accuracy of the
division was limited. Because finally the calculated pixel is rounded to an integer between 0-255
and the simulation has shown that it is not necessary to calculate highly accurate. The code of
the division and the data types used is shown in listing 4.3.
29

Chapter 4. Image Processing

1

ap_ufixed <18,1> r e c = 1 . 0 ;

2

ap_uint <14> den_Var ;

3

ap_ufixed <18,1> den ;

4

ap_int <23> num;

5

ap_int <23> div ;

6
7

den = r e c / den_Var ;

8

div = num * den ;

Listing 4.3: Calculation of the division for the Wallis filter

4.2.3

AXI4-Stream

After initialization the calculation of a new output pixel requires 21 pixels (one window length).
Due to the initial AXI4-Stream data size (8 bits), 21 clock cycles had to be waited until the
calculation could be started, since only one pixel is received per clock cycle. This also prevents
the loops from unrolling, as not all data is available at the input (see chapter 2.3.2).
The advantage of Vivado HLS is that interfaces can be adapted relatively easy and quick. The
AXI4-Stream has therefore been enlarged to receive one column per clock. This means that the
AXI4-Stream has been extended to a data width of 256 bits [18]. This also made it possible to add
the directive unroll to the loops. With this solution the throughput could be increased. Listing
4.4 shows the expansion of the AXI4-Stream interface.
1

type def ap_axiu <8 ,1 ,1 ,1 > AXI_VALUE ;

2

type def h l s : : stream<AXI_VALUE> AXI_STREAM;

/ / <TDATA, TUSER, TID , TDEST>

3
4

/ * Change t o : * /

5
6

type def ap_axiu <256 ,1 ,1 ,1 > AXI_VALUE ;

7

type def h l s : : stream<AXI_VALUE> AXI_STREAM;

/ / <TDATA, TUSER, TID , TDEST>

Listing 4.4: Calculation of the division for the Wallis filter

4.2.4

Array Partition

The pixels are sent to the Wallis IP core via AXI4-Stream. An AXI4-Stream can be imagined
as a FIFO. When the data is read, it is no longer available at the input of the AXI4-Stream.
This means the pixels have to be temporarily stored, because the pixels are used again with
every iteration (4.1). The pixels are temporarily stored in a block RAM (BRAM). But this brings
another problem. With a BRAM only 1 pixel per clock can be read. 21 pixels must be processed
per iteration. The latency would be at least 21 clock cycles if it were stored in a single BRAM.
With the directive array_partition (see chapter 2.3.2) an array can be divided into multiple
smaller arrays. The array is divided into 21 smaller arrays. So it is possible to read 21 pixels per
clock.
30

4.3. Implementation (VHDL)

In listing 4.5 the array pixel is partitioned into 21 arrays with the directive array_partition
so that 21 pixels per clock can be read.
1

s e t _ d i r e c t i v e _ a r r a y _ p a r t i t i o n − type c y c l i c − f a c t o r 21 −dim 1 " w a l l i s " p i x e l

Listing 4.5: Set directive array_partition

4.3

Implementation (VHDL)

This chapter describes the Wallis filter with the calculation of the mean and variance values in
the HW programming language VHDL. The advantages of a VHDL implementation compared to
a C/C++ with Vivado HLS is described in chapter 7.
Chapter 4.3.1 dissects the concept of the VHDL code by using a block diagram. This is divided
into the calculation of the mean and variance values, as well as the Wallis algorithm. The last
chapter describes the problems with programming the algorithm.

4.3.1

Concept

This chapter presents the concept of the mean and variance value calculation, as well as the
Wallis filter. They are shown as a block diagram and briefly explained.

Mean and Variance
Block diagram 4.3 is based on the two equations of the mean 4.1 and the variance values 4.6.
The pixel data is sent as 8 bit AXI4-Stream into the IP core. The data is received by the block

dir_shift_reg . This block is used for initialization and iteration sequence, which is shown in
figure 4.2. All pixels are passed directly to the plus output and through a FiFo with a delay of 441
pixels to the minus output (441 being the window size or the square of the window length). This
FiFo is later used to subtract the old pixels from the sum of the pixels, as shown in figure 4.2.
Figure 4.4 shows the simulation of the block dir_shift_reg. The delay of the input data can be
seen on the minus output.
In the block sum_diff the two outputs of the block dir_shift_reg are subtracted from each
other and the result is added in a sequence of 441 cycles. In the initialization sequence, the minus
output is zero and the pixels are only added. This means that a correct result is obtained after
441 clock cycles and then after 21 clock cycles if the IP core is in the iteration sequence. Figure
4.4 shows the simulation of the block sum_diff. The sum of the difference of the input data of
the dir_shift_reg can be seen at the output.
Finally, the sum of the pixels is multiplied by 1/N where N is 441 (equal to the window length).
For the variance, the square mean is subtracted. The mean and the variance are passed to the
Wallis filter.
31

Chapter 4. Image Processing

Mean & Variance
dir_shift_reg
pixel
FiFo

sum_diff
plus
minus

()2

Σ

µn

1
N

()2

()2
sum_diff

Σ

−
1
N

σ2n

Figure 4.3: Concept of the mean and variance implementation in VHDL

Figure 4.4: Simulation of the dir_shift_reg and the sum_diff block with a WIN_SIZE of 9
Wallis Algorithm
Block diagram 4.5 is based on the equations of the Wallis algorithm 2.4. The pixel to be calculated
is connected to the IP core via AXI4-Stream. The constant parameters are connected to the

controller and sent from the computer.
The division is implemented using the Divider Generator. This is an IP core by Xilinx that can
be configuered to implement different algorithms for divisions. Once the pixel is calculated, it is
sent to the controller via AXI4-Stream.

32

4.3. Implementation (VHDL)

Wallis Filter
numerator

Divider Generator

I(x, y)
µn

−

LogiCORE IP

I 0 (x, y)

c · σ2g

denumerator
σ2n

c
(1 − c)σ2g

addition
µn

1−b
b · µg

Figure 4.5: Concept of the Wallis filter implementation in VHDL

4.3.2

Hurdles

This chapter is about the hurdles in programming with VHDL and the implementation of the
mean and the variance value calculation, as well as the Wallis algorithm. The division and the
storage of fractional numbers are discussed.
Mean and variance value calculation
A division exists in the calculation of the mean and the variance. As already mentioned in chapter
4.2.2, a division on an FPGA is a complex task. This division is relatively easy to solve. As can be
seen from the equation of the two calculations (4.1 and 4.6), the divisor is constant. This gives the
possibility to calculate 1/N in advance and to store the obtained value as a constant on the FPGA.
The result consists of a multiplication with two integers on the FPGA that are interpreted as
fractionals and can be calculated in one clock cycle.
Calculated 1/441 results in ≈ 0.00226757. This number is converted to binary and results in
≈ 0.00000000100101001001101. The following number is stored as a constant 100101001001101.

Note that 8 decimal digits have been removed. This must be observed in the following calculations,
because the decimal place can change with each calculation. Figure 4.6 lists the data widths for
the calculation of the mean and the variance. The square brackets contain the entire data width.
If there are two numbers in the square brackets, the first is the entire data width and the second
is the integer width. The sign is in front of the brackets: u for unsigned and s for signed.
33

Chapter 4. Image Processing

Mean & Variance
dir_shift_reg
pixel u[8]
FiFo

sum_diff
plus u[8]
minus u[8]

()2
u[16]

Σ

s[17]

s[32, 17]

1
N

µn u[8]

u[12, 8]

()2

()2

u[16]

u[24, 16]

sum_diff

Σ

−

s[25]

1
N

σ2n u[14]

s[40, 25]

Figure 4.6: Data width of mean and variance calculation

Wallis Algorithm
The Wallis algorithm also contains a division. In this case, however, it is not possible to simplify
it because the numerator and denumerator change with each pixel.
Xilinx offers an IP core which solves divisions. It is called Divider Generator [19]. The Divider

Generator contains three different implementations of divisions (LUTMult, Radix-2, High Radix).
These three divisions are distinguished by resource consumption, latency and throughput. The
numerator and denumerator can be passed to the Divider Generator via AXI4-Stream. The
result is again sent via AXI4-Stream.
The decision was made to implement the High-Radix algorithm. Due to the size of the dividend
the LUTMult was omitted and because of the latency the implementation of the Radix-2. The
divider should accept a new division after at most 21 clock cycles, because this is the time of the
iteration sequence. This is fulfilled by the implementation of the High-Radix. The latency and
throughput of the calculation can be influenced during implementation [19].

34

CHAPTER

5

D ATAFLOW

With the Wallis filter implemented on the FPGA the problem occurs that the image data has
to be sent from the computer to the FPGA and back. The following chapter explains the realization of said dataflow. It is split into two main parts: the communication and control parts.
But before diving into them the concept of the dataflow is briefly described in the following chapter.
During the work on the project it has been discovered that the initial approach to the problem
will not lead to the optimum solution. This is why a second implementation was made that differs
in both communication and control parts. To prevent confusion these two solutions are referred
to as solution A (the first approach) and solution B (the second, more performant approach).

5.1

Concept

As seen in the introduction the image data is sent to the FPGA over Ethernet. This means that
on the FPGA side an IP stack has to be implemented to handle Ethernet communication. The
received data is sent to the Wallis filter for processing and its results are sent back to the host
PC. To distinguish the data transmission from the dataflow inside the FPGA, the dataflow was
split into two blocks: the communication and control block. The communication block handles the
Ethernet communication while the control block feeds the data in the right order to the Wallis
filter and does housekeeping work. Figure 5.1 shows the dataflow for solution A.

35

Chapter 5. Dataflow

AXI

UFT core

BRAM
AXI

control

Stream
Controller

Stream

Image
Processing

control
Figure 5.1: Dataflow inside FPGA for solution A

Image data received from the UFT core is stored directly into FPGA block memory through
AXI4 memory mapped interfaces. The UFT core signals a complete file transfer by asserting

rx_done. This is when the controller starts reading the data from memory and sending it through
AXI4-Stream to the Wallis filter. The processed data is again stored in block memory. If all data
has been processed, the controller configures the UFT core with the amount of data to send back
and starts the transmission.

Following the path of dataflow it is easy to see that the data is stored multiple times: in the
receiving FiFo of the UFT core, the block memory, then by the controller in the transmitting and
receiving path, again in block memory and in a Tx FiFo of the UFT transmitter. This causes
latency and high resource usage. Because of that, in a second approach (solution B) the datapath
was chosen more directly. Key elements are the AXI4-Stream interfaces. Figure 5.2 shows the
new approach using said stream interfaces. The UFT stack outputs received data coming from
the Ethernet line with low latency. The controller then only buffers the necessary data and starts
streaming pixel data to the Wallis filter as soon as enough data is received. Processed data is
buffered in the controller until the transmitter is ready to send data back.

Stream
UFT core

Stream

Stream
Controller

Stream

control

Image
Processing
control

Figure 5.2: Dataflow inside FPGA for solution B

36

5.2. Communication

5.2

Communication

The UFT core from project 5 serves as a starting point to the communication problem. Altough it
works on a fundamental basis, the following key features are missing:
1. Acknowledgment of received data
2. Retransmission if a packet was not received
3. Control interface
4. Stream based data interface
In the following chapters these four problems are addressed and the solutions explained. To read
more about the communication core as it was used from the preceding project the p5 project report
can be consulted [3]. Furthermore the UDP File Transfer (UFT) Protocol Specifications (appendix
A.4) and calculations (appendix A.5) give information about the communication protocol.

5.2.1

Acknowledge

The communication is based on the User Datagram Protocol (UDP). This protocol features low
overhead and is simple to implement. The main problem is that the data sent is not acknowledged
hence the sender has no feedback whether the data was received or not. This is one reason a
custom session protocol (UDP File Transfer (UFT)) was introduced in the preceding project. It
provides command packets for data packet acknowledgment. Table 5.1 lists the UFT commands.
The UFT core was extended with the functionality to send acknowledges back to the PC for data
packets and file transfers. For this to work, new connections inside the UFT block were made.
Drawn red in figure 5.3, the uft_rx_mem_ctl block signals the uft_tx_ctl block to send an
acknowledgment. The connection consists of the signals listed in table 5.2. The tx control block
latches the two request signals ack_cmd_nseq and ack_cmd_ft in the case a transmission is
running and an acknowledge request is made. If the tx state machine enters idle or wait state
it checks these latches for an acknowledge request. If a request is latched, the tx command
assembler and tx arbiter are turned on to send an acknowledge packet. After the packet was sent,
the according ack_cmd_nseq_done/ack_cmd_ft_done signals are asserted.

Command

Short

Name

Data 1

Data 2

0x00
0x01
0x02
0x03

FTS
FTP
ACKFP
ACKFT

File transfer start
File transfer stop
Acknowledge data packet
Acknowledge file transfer

TCID
TCID
TCID
TCID

NSEQ
0x0000 0000
SEQNBR
0x0000 0000

Table 5.1: UFT command list
37

Chapter 5. Dataflow

uft_top
uft_rx

uft_rx_mem_ctl

rx_hdr
s_axi

axi_master_burst
AXI

s_axi

uft_tx_top
uft_tx_ctl

tx_hdr
controll

uft_tx_cmd_assembler
s_axi
uft_tx_arbiter
axi_master_burst

s_axi

uft_tx_data_assembler

AXI

s_axi

Figure 5.3: UFT Top Block Design with acknowledge path

Signal

in/out

width

Description

ack_cmd_nseq
ack_cmd_ft
ack_cmd_nseq_done
ack_cmd_ft_done
ack_seqnbr
ack_tcid
ack_dst_port

out
out
in
in
out
out
out

1
1
1
1
24
7
16

ack_dst_ip

out

32

Command to send a data packet acknowledge packet
Command to send a file transfer acknowledge packet
Asserted if the data packet was acknowledged
Asserted if the file transfer was acknowledged
What sequence number to acknowledge
What transaction id to acknowledge
Destination port of the host to send the acknowledge
to
Destination IP address of the host to send the acknowledge to
Table 5.2: ACK signals

5.2.2

Retransmission

Now that the FPGA sends an acknowledge packet for each data packet, the sending PC can
check if all packets were received by the FPGA. Therefore the PC software was extended with
retransmission. Before sending data an array is allocated that will hold the information if a
packet was received. Each element represents a data packet. Listing 5.1 shows the allocation of
the acknowledge buffer.
1
2

u i n t 8 _ t * ack_buf = ( u i n t 8 _ t * ) malloc ( nseq * s i z e o f ( u i n t 8 _ t ) ) ;
memset ( ack_buf , 0 , nseq * s i z e o f ( u i n t 8 _ t ) ) ;

Listing 5.1: ack buffer allocation

38

5.2. Communication

During transmission, the sender checks the receiving buffer for data. If an acknowledge packet has
been received, the acknowledge array is updated. This allows the sending routine to retransmit
packets that were not acknowledged. Listing 5.2 shows the acknowledge buffer update after
packet is received.
1

Recv ( sockfd , buf , 1500 , 0 ) ;

2

i f ( get_command ( buf ) == CONTROLL_ACKFP)

3

{
ack_buf [ get_command_ackfp_seqnbr ( buf ) ] = 1 ;

4
5

}

Listing 5.2: ack update

5.2.3

Control Interface

The data received by the UFT core in solution A is stored in memory and data to be sent back is
also located in FPGA memory. To instruct the core to send data, a control interface is required.
Tx base address, data size and rx base address are values that will be sent from the controller to
the UFT core as described later in chapter 5.3. For this purpose an AXI4-Lite slave interface was
realized. AXI4-Lite is a stripped-down version of AXI4. It allows single beat, memory mapped
read and write access from master to slave [13].
Using Vivados Create and Package New IP command, a 16 register AXI4-Lite slave interface
was generated and connected with the according control signals of the UFT core. Table 5.3 shows
the register map. Registers 8 through 15 can be written by the PC using a custom UFT command
packet. This allows parameters to be sent from PC to FPGA.
This new control interface allows the control and status signals from the UFT core to be read
and written using a single normed interface. Figure 5.4 shows the complete UFT IP core with
AXI4-Lite control interface, stream and control interface to the UDP core and AXI master burst
interface for memory access (refer to [3] for AXI master burst).

Figure 5.4: UFT core with AXI4-Lite interface
39

Chapter 5. Dataflow

Nr

R/W

Offset [hex]

Description

0

RO

00

1
2

WO
WO

04
08

3

WO

0C

4
5
6
7
8
9
10
11
12
13
14
15

RO
WO

10
14
18
1C
20
24
28
2C
30
34
38
3C

Status register. Bit 0: tx_ready. Set if transmitter is ready to
send data.
Control register. Bit 0: tx_start. Set 1 to start transmitter.
Receiver base address. Received data is stored starting from
this address.
Transmitter base address. Data to send is read from this
address.
Rx counter. Counts how many file transfers were received.
Tx size. How many bytes to send.
Not used
Not used
User register 0
User register 1
User register 2
User register 3
User register 4
User register 5
User register 6
User register 7

RO
RO
RO
RO
RO
RO
RO
RO

Table 5.3: UFT core register map. RO=read only, WO=write only

5.2.4

Data Interface

The UFT core was in the first place intended for file based data transfer from PC to FPGA and
vice versa. This led to the decision to use memory mapped AXI interface to store the received
data and read the data to be sent. It made sense with the UFT protocol being file oriented. In the
progress of developing the Wallis filter, a stream based approach was pursued to reduce latency
and memory usage. With the UFT core being memory based, a controller had to be introduced to
send the data from memory to the Wallis filter. Early tests foreshowed that the UFT core with its
memory based interface would be the limiting member in the data processing chain. So the core’s
data interface was rewritten to support AXI4-Stream for data receiving and sending.
For solution B, the two axi_master_burst blocks were removed and the code of the uft_rx_mem_ctl
and uft_tx_data_assembler was changed to directly connect the AXI4-Stream from the UDP
stack to the outside. Advantage of this solution is the reduced latency and less memory usage.
One downside is that the receiver is no longer able to reorder packets if they do not arrive in
order. As long as the application runs on a closed network it can be assumed that packets will not
change order. The changes made for the streaming interfaces are shown in figure 5.5.

40

5.2. Communication

uft_top
uft_rx
rx_hdr
s_axi

uft_rx_mem_ctl

axi_master_burst
s_axi_rx

uft_tx_top

s_axi

uft_tx_ctl
tx_hdr
controll

uft_tx_cmd_assembler
s_axi
uft_tx_arbiter
axi_master_burst

s_axi

uft_tx_data_assembler

s_axi_tx

s_axi

Figure 5.5: UFT Top Block Design with AXI4-Stream interface

41

Chapter 5. Dataflow

5.2.5

Implemented Features

In the final version of the UFT core used in solution B the AXI4-Lite interface was removed
because no memory addresses had to be exchanged and the solution B controller was written in
VHDL. Adding an AXI4-Lite master interface would have only made it more complicated.
To conclude the changes made to the UFT core from the version of project 5:
1. Added acknowledgment on receiver path
2. Changed data interfaces from memory mapped to AXI4-Stream
3. Added user register to exchange configuration data
4. Bug fixes
There are still some features that are not yet implemented but are defined in the UFT protocol
specifications:
1. Acknowledge check on transmit
2. Retransmission during send
Figure 5.6 shows the IP core with the AXI4-Stream ports.

Figure 5.6: UFT IP core with AXI4-Stream interface

42

5.3. Control

5.3

Control

Now that the image data is received by the UFT core it has to be sent to the Wallis filter in the
right order. Reordering the pixel data is the main task of the controller besides caching data and
controlling the UFT and Wallis cores. This chapter describes the tasks to be solved.
As already mentioned at the beginning of chapter 5, two implementations were realized. Solution
A was implemented using Vivado HLS and is documented in chapter 5.3.2 followed by solution B
written in VHDL and described in detail in chapter 5.3.3.

5.3.1

Concept

Image data coming from the PC is sent row wise with the left most pixel sent first. This is due
to the fact that OpenCV (which is used on PC side to access images) stores the image data in
said layout [20]. The Wallis filter however, requires its input data to be column wise with the top
most pixel sent first as described in chapter 4.1. Figure 5.7 illustrates this issue. The blue “write”
arrow shows the data coming from the UFT core and the red “read” arrow represents the order
the data has to be sent to the Wallis filter. For illustration purposes a window length of five and
image width of eight is used.
Read
Write

0
1
2

Window length

3
4

Image width

Figure 5.7: Memory reordering problem

43

Chapter 5. Dataflow

Read

0
1
2

Window length

3
4

Image width

Figure 5.8: Scenario A) Data growth in horizontal direction

This is only one part of the problem. Another issue arises if the progression of the neighborhood
across the input image is observed. We distinguish two scenarios: A) moving the neighborhood
horizontally to the right and B) advancing the neighborhood to the next row vertically. The first
scenario is illustrated in figure 5.8. The green arrow marks the new column of the neighborhood
to be sent to the Wallis filter.
This progression is made for each column of the input image until the last column was processed.
Then scenario B) comes to play. Moving the neighborhood vertically to the next row by one pixel
requires new image data of one row and W I NDOW_LENGT H − 1 rows of image data that had
already been processed on the previous row. Figure 5.9 illustrates scenario B.

Read
0
1
2
3

Window length

4
Write

5

Image width

Figure 5.9: Scenario B) Data growth in vertical direction

44

5.3. Control

From these two scenarios we can conclude two problems the controller has to solve:
1. Send the image data in the right order (column wise with W I NDOW_LENGT H sized
columns)
2. Cache W I NDOW_LENGT H − 1 rows for the computation of the next image row
In addition the controller has to start the UFT transmission if an image row has been processed
and configure the Wallis filter with the parameters coming from the UFT core.

5.3.2

Implementation HLS

Now that the problems are analysed, the proceeding can be stated. For the first solution an
approach using Vivado HLS was picked. The main motivations for this decision were the fact
that we had learned how fast we can have a working IP core using the Vivado HLS workflow and
that we wanted to test the ability to implement a state machine in Vivado HLS. In the following
chapters the requirements for this state machine are listed, a brief insight into the source code is
given and the main hurdles while developing the core are unfolded.
Requirements
Besides the dataflow described in 5.3.1 the IP core interfaces must be defined. Table 5.4 lists
the IP interfaces as defined in controller.cpp. The IP core implements a finite state machine
as seen in figure 5.10. After the initial state INIT where the UFT core is initialized, the state
machine goes to its idle state IDLE. If the UFT core indicates the end of a file transfer the image
width is stored and the state machine switches to the READ state. There it fills the first junk of its
buffers and switches to the STREAM state. Now the data is sent from the internal buffers to the
Wallis filter in the correct order. Simultaneously the processed pixels from the Wallis filter are
stored inside a memory.

Variable

Type

Connection to

Description

memp

AXI4 Master

Memory

cbus

AXI4 Master 1

UFT core

inData
outData
rx_done
tx_ready

AXI4-Stream
AXI4-Stream
ap_uint<1>
ap_uint<1>

Wallis filter
Wallis filter
UFT core
UFT core

Access to the block memory where the image data
is stored
Control the AXI4-Lite slave registers to control
the UFT core
Processed pixels coming from the Wallis filter
Image data sent to the Wallis filter for processing
Is asserted after a file transfer is complete
Is asserted if UFT core is ready to send

Table 5.4: Controller solution A interface ports

1 Vivado HLS does not support AXI4-Lite master, but a AXI4 can be converted to AXI4-Lite

45

Chapter 5. Dataflow

READ

start

INIT

STREAM

WRITE

IDLE

SEND

WAIT_TO_
SEND

Figure 5.10: Controller solution A) simplified state machine

If all pixels in one line are processed the state WRITE is activated. The processed pixels are stored
in block memory using the memp AXI4 Master interface and the state machine switches to the

WAIT_TO_SEND state. If the UFT core Tx is ready, the state SEND is activated where the UFT core
is configured and a transmission is started.
Realization
According to Xilinx application note XAPP-1209 [21] a state machine in Vivado HLS can be realized by using a switch statement in the C/C++ code. The code in file controller.cpp implements
the state machine behavior. The state machine code is very straightforward. More work went
into the memory layout. Because the memory bus to read and write pixels to and from memory
is AXI4 based, multiple single byte access result in considerably less throughput than single
burst access. To take advantage of these burst accesses, a ping-pong buffer structure was realized.
There are two buffers, each of the size of W I NDOW_LENGT H ∗ A X I_BU RST_SI ZE bytes,
where A X I_BU RST_SI ZE represents the number of bytes read in one burst access. It requires
W I NDOW_LENGT H burst reads to fill one buffer. If one buffer is filled, it can be accessed by
single byte access to read the data in the required order for the Wallis filter, as shown in figure
5.7. During the time the data is read from one buffer, the other buffer is filled with image data
from the AXI4 bus, hence the name ping-pong buffer.
Figure 5.11 shows the memory layout of solution A. The blue and red rectangles represent the
two buffers. Visualized is an AXI burst length of eight and a window length of five.

46

5.3. Control

0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

0
1
2

Window length

3
4

AXI burst length

Figure 5.11: Memory layout for solution A

Filling the buffer is done in the fillBuff routine as shown on listing 5.3.
1

uint32_t i , inOff , outOff ;

2

f o r ( i = 0 ; i < WINDOW_LEN; i ++) {

4

i n O f f = o f f + i * imgWidth ;
outOff = i *AXI_BURST_SIZE ;

5

memcpy(&buf [ outOff ] , &memp[ i n O f f ] , AXI_BURST_SIZE ) ;

3

6

}

Listing 5.3: Buffer fill simplified
To send the right order of output pixels, two counters are required. The first counter named

ms_rctr increments after every pixel sent and wraps at W I NDOW_LENGT H. Therefore it
counts the current column. The second counter named ms_cctr increments every time the

ms_rctr wrappes around and counts up to I M AGE_W IDT H. From these two counters the
address of the pixel to be sent can be calculated as shown in listing 5.4.
1

oPxl . data = outPpBuf [ AXI_BURST_SIZE * ( ms_rctr ++) + ms_cctr ] ;

Listing 5.4: Pixel send address calculation
Receiving a pixel from the Wallis core is simply storing the pixel at the next memory location of
the output buffer. Listing 5.5 shows the receiveing of processed pixels.
1

i P x l = inData . read ( ) ;

2

out_mem [ sm_ctr ++] = ( u i n t 8 _ t ) i P x l . data ;

Listing 5.5: Pixel read store
Every time a pixel is read or written a corresponding counter is incremented. After both counter
hit their compare value, the stream state is exited and the data is copied back to memory.
47

Chapter 5. Dataflow

Hurdles
The C/C++ code was written in little time. With less than 300 lines it is a manageable amount of
code. The C/C++ and RTL-simulation yielded no errors but the implemented core together with
the Wallis filter core did not work as expected. Thanks to the integrated logic analyzer (ILA) the
signals connecting the controller and Wallis core could be inspected. Soon it was discovered that
the controller only output one pixel and then stopped. It was discovered that this was due to
the fact that a read() operation on a AXI4-Stream in Vivado HLS is blocking, meaning that the
synthesis tool implements the code in a manner that if no data is valid on the stream, the code
will wait until there is data on the stream. This led to the code waiting for an input byte and no
output was generated. This issue was fixed by wrapping the read access with a query weather
the input stream holds data. Listing 5.6 shows the adjusted pixel read. The same applied to the
sending of data. It is first checked whether the output stream is not full before sending new data.
1

i f ( ! inData . empty ( ) ) {
i P x l = inData . read ( ) ;

2

out_mem [ sm_ctr ++] = ( u i n t 8 _ t ) i P x l . data ;

3
4

}

Listing 5.6: Pixel read store with query

Conclusion
To conclude the findings using a Vivado HLS based approach, the thesis that with little effort
a working solution can be produced is confirmed. The required interfaces (AXI4-Master and
AXI4-Stream) were implemented with a single line of code and the statemachine with a simple
switch statement. After clearing some hurdles the controller core worked as expected with two
drawbacks:
1. Throughput: The controller core outputs data at a rate of one byte every 9 clock cycles as
can be seen in the result of the simulation. In theory, using a block memory as internal buffer,
it should be possible to output one byte per clock cycle. This issue could not be resolved in a
forseeable time. The analysis view of Vivado HLS would give indications on what calculation is
resulting in a reduced throughput but the analysis results could not be interpreted.
2. Memory layout: In solution A the image data is stored multiple time as already mentioned
in chapter 5.1. The data is stored twice in the controller core alone: during read in the ping-pong
buffer and during write in the output buffer. This increases the resource usage.
These two issues were the motivation to rethink the memory layout and functionality of the
controller core which led to solution B, a controller implemented in VHDL.

48

5.3. Control

5.3.3

Implementation VHDL

In a new implementation of the controller, the two drawbacks of solution A are addressed. The
following chapter is structered likewise starting with the new requirements, how the core was
realized, the hurdles across the way and a conclusion of the performance of the new controller
core.
The Wallis filter core was built from ground up with AXI4-Stream interfaces. A slave interface
for input data and a master interface for output data. This stream based approach should be
implemented in the controller as well to reduce buffering to a minimum to decrease memory
usage. The use of streaming interfaces required a new memory structure. These two aspects form
the requirements of the new controller core.
Requirements
With the new stream based approach the UFT communication core first had to be altered to
support stream interfaces. This is explained in chapter 5.2.4. With the AXI4-Lite configuration
interface removed, the control signals are replaced with arbitrary signals. The new interface
definition is dissected in table 5.5.
Furthermore the order of pixel to be sent to the Wallis filter remain the same as described in
chapter 5.3.1. A stream based approach also eliminates the need for a statemachine. The entire
controller is to be stateless.
Name

Type

To

Description

uft_i_axis
uft_rx_done
uft_user_regX

AXI4-Stream
arb
arb

UFT Rx
UFT Rx
UFT Rx

Receive data stream from UFT core
Signals a complete UFT transfer
UFT user register values. They can be set from
PC and serve as channel for Wallis parameters

uft_o_axis
uft_tx_start
uft_tx_ready
uft_tx_data_size

AXI4-Stream
arb
arb
arb

UFT Tx
UFT Tx
UFT Tx
UFT Tx

Transmit data stream to UFT core
Asserted to initialize a transmission
Signals that the UFT core is ready to transmit
How many bytes to transmit

wa_i_axis
wa_o_axis
wa_par

AXI4-Stream
AXI4-Stream
arb

Wallis
Wallis
Wallis

Image input data to the Wallis core
Processed image data from the Wallis core
Parameters for the Wallis filter

Table 5.5: Controller solution B interface ports (unlisted interfaces are not used)

49

Chapter 5. Dataflow

dc_top
dc_control
uft control

wallis control
dc_mmu
wallis data in

uft rx
axis_fifo

wallis data out

uft tx

Figure 5.12: Controller solution B) block diagram

Realization
The controller core consists of three entities joined in a top entity. Figure 5.12 shows a block
diagram of the controller. The dc_control entity takes control of housekeeping such as initiating
a UFT transmission after complete processing and resetting all instances on a new input image.
The processed pixels are buffered in the axis_fifo. This is necessary because if an Ethernet
transmission is started, every clock a byte has to be valid. The third entity, dc_mmu is where the
image data is cached and sent to the Wallis filter in the right order, hence the name memory
management unit.
The memory management unit works similar to a large FiFo with the addition that elements
can be read multiple times. To explain its functionality, the operation of a simple FiFo has to be
clarified. A FiFo has three main components: the memory, a read pointer and a write pointer.
Figure 5.13 shows the structure of a FiFo. Five states can be identified. State I is the initial
state after reset, both read and write pointers point to the first element in the array. If a write
occurs, the write pointer advances which is observed in state II. The filled elements in the array
represent values that can be read. As long as the write pointer is ahead of the read pointer,
data can be read. The next state appears when the write pointer wraps around the length of the
memory to the first element as shown in state III. After several read accesses the read pointer
wrappes around as well and we can observe state IV which is equivalent to state II. If the read
pointer caught up to the write pointer state V is observed.

50

5.3. Control

State:
Looped:

I
false

II
false

w

r

w

III
true
w

r

r

IV
false

V
false

w

w

r

r

Figure 5.13: FiFo structure

From these five states we can conclude conditions for when a read or write access is permitted. To
do this a variable named looped is introduced. It is set if the write pointer wraps around (as seen
in state III) and clear if the read pointer wraps (state IV). Listing 5.7 shows the read condition.
1
2
3

i f ( ( looped = true ) or ( w_ptr / = r _ p t r ) ) then
DataOut <= Memory ( r _ p t r ) ;
end i f ;

Listing 5.7: FiFo read condition
Similarly, listing 5.8 shows the write condition.
1
2
3

i f ( ( looped = f a l s e ) or ( w_ptr / = r _ p t r ) ) then
Memory ( w_ptr ) : = DataIn ;
end i f ;

Listing 5.8: FiFo write condition
Now that the mechanics of a FiFo are dissected, the memory layout of the MMU can be illustrated.
Figure 5.14 shows the memory layout. At the core of the memory management unit is a two
dimensional array (it is implemented as a simple array in VHDL for better synthesis results).
Every line represents an input image line and every column an input image column. The idea is
to have a full block RAM for each line which is why from now on a line is called cache. The input
image data is received line by line. Each input line is stored in a cache.

51

col_w_ptr

Chapter 5. Dataflow

0

cache_w_ptr

1

2

3

4

5

6

0

7

8

cache_r_base

1

cache_r_ptr

2
Cache N Lines

3
4

cache_r_tip

col_r_ptr

5

Block RAM size

Figure 5.14: Memory layout for solution B

If at least W I NDOW_LENGT H lines are received, there is enough data in the memory to start a
Wallis operation for one input line. If one input line is processed the Wallis neighborhood window
slides one pixel downwards. Observed in the Y-axis (top to bottom) this mechanism resembles a
FiFo with cache_w_ptr being the write pointer and cache_r_ptr representing the read pointer.
The major difference to a simple FiFo is that the distance between the read and write pointer
may not become zero because the Wallis filter requires W I NDOW_LENGT H lines to process
data.
The write mechanism consists of two pointers. The cache_w_ptr pointer points to the current
cache in which the input line is stored. The col_w_ptr points to the location in the cache of
the current pixel to be stored. col_w_ptr is incremented every pixel and set to zero at the
start of a new line. cache_w_ptr is incremented if an image line is received and wraps at
C ACHE_N_LI NES.
The read mechanism consists of a total of four pointers. The col_r_ptr pointer points to the current image column that is sent to the Wallis filter. It is incremented after W I NDOW_LENGT H
pixels are sent and set to zero at the beginning of a new line. cache_r_base points to the top
most cache of image data and cache_r_tip to the bottom most. They are both incremented after
an image line is processed and wrap at C ACHE_N_LI NES. The cache_r_ptr pointer points
to the current cache from which the pixel is read and increments if a pixel is sent to the Wallis
core. It wraps to zero if it reaches C ACHE_N_LI NES and wraps to cache_r_base if it reaches

cache_r_tip.
52

5.3. Control

With the pointer mechanics dissected, new conditions for read and write can be concluded. Listing
5.9 shows the read condition. The difference to the simple FiFo condition is that if not looped
the write pointer must be greater than the read tip. This ensures that always enough data is in
memory for a Wallis operation.
1

i f ( ( looped = true ) or ( cache_w_ptr > c a c h e _ r _ t i p ) ) then
DataOut <= Memory (BRAM_SIZE* cache_r_ptr+row_r_ptr ) ;

2
3

end i f ;

Listing 5.9: MMU read condition
Listing 5.10 shows the write condition. It only differs in that the write pointer may not be equal
to the read base instead of the read pointer as in the case of the simple FiFo.
1

i f ( ( looped = f a l s e ) or ( cache_w_ptr / = cache_r_base ) ) then
Memory (BRAM_SIZE* cache_w_ptr+row_w_ptr ) : = DataIn ;

2
3

end i f ;

Listing 5.10: MMU write condition
The looped flag is set if cache_w_ptr wraps to zero and is cleared if cache_r_tip wraps to zero.
These read and write conditions are used to access the memory and to drive the side channels
of the AXI4-Stream (tready and tvalid). The tlast signal is used to indicate the end of a line.
Therefore the MMU can operate with an input and output stream and information about the
image width and window length.
The VHDL code for the memory management unit is mainly composed of processes. Each pointer
is incremented and wrapped in its own process. The memory read and write accesses are described
in processes as well. Finally there are two more processes: p_out_pix_ctr counts the number of
pixels sent and p_out_pix_m1 calculates the number of output pixels to send.
Hurdles
After figuring out how to increment and wrap which pointer and when to allow reads and writes,
the memory management unit worked well in simulation. The dc_control and axis_fifo block
were quickly implemented. The core was tested using VHDL testbenches. A big advantage was
that the controller and Wallis core could be simulated before being implemented on the FPGA
which made fault finding much faster. With this new stateless approach timing issues began to
arise.
Address calculations had to be done within one clock cycle. While Vivado HLS optimizes variables
and datapaths, this has to be done manually in a VHDL based approach. For example on critical
path was the calculation of n_out_pix_m1. This signal is required to compare the output pixel
counter and to signal the last pixel sent to the Wallis core. The calculation is shown in equation
5.1.
n out_ pix_m1 = img_width · win_l en − 1
53

(5.1)

Chapter 5. Dataflow

The parameters win_l en and img_width were chosen of a size that the multiplication could
be implemented in a DSP block. The subtraction of one is also done in the same DSP block.
Synthesis results showed that this DSP calculation requires approximately 3.3ns which is almost
already half of the targeted clock period of 8ns. Together with the datapath and following logic,
this resulted in a critical path exceeding 8ns. To fix this, this calculation had to be done in a
clocked process so that a latch is inferred. Such timing issues had to be fixed for several signals.
Conclusion
Because the requirements were already dissected from solution A there was only a new memory
layout to be realised for solution B. With the main difference being that now the behavior was not
described in C/C++ language but in VHDL. The total amount of time spent on the solutions were
approximately the same. What had changed is on what the time was spent. While writing the
code required the same amount of time, debugging and fault finding differed. The HLS solution
required time in understanding how the synthesis tool translated the C/C++ code (referring to
the AXI4-Stream blocking access problem) while it did not require thoughts on timing and critical
paths. With VHDL these accesses had to be implemented manually and worked on first try but
some time had to be spent on fixing timing issues.
Performance wise, there is the expected increase in throughput. Output pixels can be sent to the
Wallis filter with every clock cycle as soon as there is enough image data in the cache. Figure
5.15 shows the output data stream with a W I NDOW_LENGT H of three.
Another advantage of this solution is that the Wallis and controller components could be simulated
together before implementation. This was not possible with the HLS based approach. Most of the
issues could be resolved during simulation.

Figure 5.15: Solution B output data stream

54

CHAPTER

6

S CALABILITY

The code for this project is written from the beginning with the possibility to scale it up. The
main idea is that the throughput can be increased. Scalability in regard to this project can be
divided into two terms:
1. Inside FPGA
2. Across multiple FPGAs
In the following sections these two aspects are discussed theoretically. For calculations the VHDL
controller and VHDL image processing core is used with the metrics shown in table 6.1.
Parameter

Value

Description

bw

1 clk = 125M p/s

be
wl

125MB/s
21

p

Input throughput of image processing core at 125MHz clock
frequency in pixels per second. (1p ≡ 1B)
Ethernet throughput
Window length of neighborhood

Table 6.1: Parameters used for scalability proposal

55

Chapter 6. Scalability

6.1

Inside FPGA

Every FPGA has a given amount of resources (LUTs, memory and so forth, as described in table
3.2). The inside FPGA scalability aims towards optimal usage of these resources. Before methods
for scalability can be compared it must be clarified what can be scaled. The project consists of
three main parts: the communication, controller and image processing parts. The communication
part is only implemented once to handle the communication to the PC and therefore can not be
scaled. The image processing part however can be implemented multiple times inside the FPGA
to increase throughput. Therefore, the controller part splits up the incoming image data and
distributes it to the image processing cores. To summarize the inside FPGA scalability:
1. Implement the communication part once
2. Implement the image processing part as many times as the available resources allow it
3. Adapt the controller to distribute data across the processing cores
Figure 6.1 clarifies the concept. Note that all image processing cores are equal.
Now that the concept is clarified, two possible solutions are compared to distribute the image
data across the image processing cores, proposal A and B. They differ in the way in what order
the data is sent to the FPGA and how it is cached by the controller inside the FPGA. For each
proposal the following metrics are calculated.
FPGA
PC

Communication

Image
Processing

High bandwidth
Low bandwidth

Image
Processing
Controller and
Image Cache

Image
Processing

Figure 6.1: Block diagram of a inside FPGA scaled solution

56

6.1. Inside FPGA

Initial size s i
The number of pixels that have to be sent to the FPGA before beginning iterational
operation.
Iteration size s r
How many pixels that have to be sent to start a new iteration.
Store size s s
How many pixels are cached inside the FPGA.
Number of inits per image n i
Denotes the number of initial data transfers of size s i that have to be made per image.
Total tx size s tx
The total number of pixels sent to the FPGA to calculate one image.
Number of image processing cores N
The number of image processing cores implemented on the FPGA.
Table 6.2 explains two proposals with a simplified window length of wl = 3 and two image
processing cores (N = 2). To understand the procedure of the Wallis core, chapter 4.1 should be
consulted.

57

Chapter 6. Scalability

Proposal A
1

2

3

4

5

6

7

8

0

0

1

2

3

4

5

6

7

8

0

1

0 c0

1 c0

2 c0

3 c0

4 c0

5 c0

6 c0

1

0 c0

1 c0

2 c0

3 c0

4 c0

5 c0

6 c0

2

0 c1

1 c1

2 c1

3 c1

4 c1

5 c1

6 c1

2

0 c1

1 c1

2 c1

3 c1

4 c1

5 c1

6 c1

3

3

4

4

5

5

6

6
Init data
N cx

Iteration data

Nth output pixel of core x

Concept
Image data is sent line wise. Each image
processing core calculates one output line,
starts as soon as enough data is transferred
and jumps N lines downwards after line
completion. Initialization transfer is done
once per image. The image cache stores wl +
(N − 1) full image lines.

c0

c1

Image processing cores 0 and 1

c0

c1

Processed output pixels

Image data is sent column wise with wl +
(N − 1) pixels per column. All image processing cores start processing at the same time
and progress N lines downwards after completion. An initialization transfer has to be
done every time all cores finish their line.
The image cache stores only the currently
required pixels for the Wallis cores.

Initial size
s i = i w (wl + (N − 1))

s i = wl (wl + (N − 1))

Iterarion size
sr = i w

s r = wl + (N − 1)

Store size
s s = i w (wl + (N − 1))

s s = wl (wl + (N − 1))

Number of inits per image
ni = 1

ni =

Total tx size
s tx = i w · i h

s tx = n i s i + (i w − wl )s r n i

1
N (i w − w l + 1)

Table 6.2: Inside FPGA scalability proposals

58

wl + (N − 1)

0

Proposal B

6.1. Inside FPGA

Before comparing the two proposals another metric has to be taken into consideration: The
effective throughput of the image processing core. The way the Wallis filter works is that it
requires a neighborhood of pixels. For each line the core processes, the initial neighborhood has
to be sent. Equation 6.1 derived in appendix A.7.2 is used to calculate the effective throughput b r
of a Wallis filter core. To simplify, the image height i h is considered to be much larger than the
window length wl and therefore the equation can be noted as shown in equation 6.1. Equation
6.1 makes sense as the Wallis filter core requires wl pixels on the input to calculate one pixel on
the output. Therefore, the output throughput (or real throughput) b r is by a factor of wl smaller
than the input throughput b w . Using the throughput of the VHDL Wallis filter implementation
(b w = 125M p/s) and the window length wl = 21 the real throughput b r is calculated:

br ≈

b w 125M p/s
= 5.95M p/s
=
wl
21

(6.1)

Under the assumption that the real throughput b r scales proportionally with the number of
image processing cores implemented, equation 6.2 can be noted to represent the total image
processing throughput b t .

bt = N · br

(6.2)

Considering the maximum throughput of the Gigabit-Ethernet connection of b e = 125MB/s

1

equation 6.3 can be derived to calculate the maximum number of image processing cores that can
be implemented before the Ethernet communication link is saturated.

be ≥ bt = N · br ⇒ N ≤

b e b e wl
≈
= wl = 21
br
bw

(6.3)

To calculate the remaining block memory that is used for the image cache, the results from
implementing the VHDL solution 2 are subtracted from the total available memory. Table 6.3
lists the required block memory usage.
The remaining block RAM that can be used for image caching inside the controller core to
reduce multiple transmission of image data is 340.5 block RAM tiles with 36Kb each resulting in
1’532KB. Table 6.4 summarizes the parameters that are used to compare the two inside FPGA
scale proposals.

1 True Ethernet throughput is less than 125MB/s considering packet overhead
2 Some of which are listed in table 7.5
3 One 36Kb BRAM tile can be used as two 18Kb RAMs

59

Chapter 6. Scalability

36Kb BRAM tiles 3

Item
Total available
TEMAC support
UDP IP core
21 Wallis cores

365
-2
-1.5
-21

Free

340.5

Table 6.3: FPGA block memory budget

Parameter
N
sb
wl

Value
21
1’532KB
21

Description
Number of image processing cores to implement
Block memory storage available for image caching
Window length of the Wallis filter

Table 6.4: Parameter summary

Now the two proposals can be put in contrast. Starting with figure 6.2 the two proposals are
compared against each other with an input image of fixed height and variable width. This was
chosen because with the main limiting factor being BRAM tile usage, the store size s s has to be
observed and it is for both proposals independent of image height i h . The image height i h is set
to 10’000 arbitrary. The two proposals can be differentiated in that proposal A is optimized for
low tx size s tx (and therefore optimized for throughput) while proposal B requires less memory
(area optimized).

Tx size with variable image width
·109

BRAM usage with variable image width
proposal A
proposal B
available

proposal A
proposal B
image size

8
6

400
s tx [p]

BRAM tile usage

600

4

200
2
0

0
1

2

3

4
i w [p]

5

6

7
·104

1

2

3

4
i w [p]

5

6

7
·104

Figure 6.2: Comparing proposals with variable image width (wl = N = 21, i h = 100 000)
60

6.2. Across FPGA

The BRAM usage is best utilized by proposal A at approximately i w = 32K p4 while the necessary
tx size is kept at the minimum. Its one disadvantage is the limited image width of 32Kp. Three
workarounds can be considered to raise the limit in image width:
1. Use less image processing cores. This would decrease the required block memory and the
image width can be increased. However, by decreasing the number of image processing
cores, the total throughput will also drop.
2. Use proposal B. Its BRAM usage is significantly smaller however the tx size will increase.
3. Split the image on the PC and process each column as if it was a single image.
This problem leads to the next term in scalability: The scalability across multiple FPGA.

6.2

Across FPGA

Now that one FPGA is capable of processing image data at a maximal rate given by the Ethernet
link, the solution can be scaled on an Ethernet network. This was the main reason to use
Ethernet as communication basis. An input image is split into parts on the computer and
processed individually by FPGAs. The processed pixels are merged together on the PC to form
the output image. Figure 6.3 illustrates a column wise image splitting to three parts. The colored
boxes represent one Wallis filter processing a single line with the filled boxed representing the
processed output pixels.
For the Wallis filter operation, the split image data has to be overlapped between the segments
to omit the loss of pixels where the image is cut. To calculate the throughput of a distributed
FPGA image processing solution, this multiple sending of data has to be taken into consideration.
b

FPGA 1

wl
2 c

FPGA 2

FPGA 3

iw
Figure 6.3: Image partitioning for distribution across three FPGAs
4 32 is chosen because it is a power of two

61

ih

Chapter 6. Scalability

Equation 6.7 derives how the total line size s l is calculated in reference to the number of FPGAs
NF the image is distributed to. With the image width i w being much greater than the window
length wl , 6.7 can be approximated.

sl = i w

(NF = 1)

wl
= i w + (NF − 1) · 2 · b c
2

(6.4)
(6.5)

= i w + (NF − 1)(wl − 1)

(wl = 2k + 1, k ∈ N)

(6.6)

(i w À wl )

(6.7)

≈ iw

Concluding this finding it can be inferred that the throughput of a distributed image processing
solution scales proportionally to the number of FPGAs the image is distributed to:
b t ∝ NF

(6.8)

≈ NF · b e

bt =
NF =
be =

(6.9)

Total throughput of distributed image processing
Number of FPGAs used
Throughput of Ethernet connection to the FPGA

A last thought is put on the network infrastructure that is required to process images distributed.
The network link of 1Gb/s on the FPGA’s Ethernet port is saturated. Therefore, if multiple FPGAs
are used on a network, the interconnecting network switch must be capable of an uplink to the
PC or server with a multiple of the throughput of the FPGA downlink. Figure 6.4 illustrates
a network where 10 FPGAs are connected to a gigabit switch that features an uplink port
supporting 10Gb/s throughput. An example would be the GS418TPP switch [22]. 16 gigabit ports
could be used to connect to FPGAs and the SFP uplink port using fibre or copper to connect to a
PC.
10Gb/s uplink

PC/server

FPGA

1Gb/s downlink

10Gb/s

10G switch

FPGA

FPGA

NF = 10

Figure 6.4: Network topology required to distribute workload onto multiple FPGAa

62

6.3. Conclusion

6.3

Conclusion

By scaling the Wallis filter operation inside the FPGA full advantage can be taken of the available
Ethernet bandwidth. The controller core thereby caches as many input lines as can be fitted into
block memory to reduce multiple transmissions of image data and increasing link efficiency. By
dividing the input image into multiple columns it can be distributed onto multiple FPGAs on a
network. This scalability across FPGAs is only limited by the network infrastructure.
Taking a 1500Mp input image as specified in the technical requirements (see appendix A.2) and
a 4:3 aspect ratio the image width i w and height i h can be calculated. The resulting image width
is split into NF columns. Table 6.5 shows the total processing time and throughput for one to
four FPGAs. With only one FPGA used the image is too wide. It is split into two columns and
processed one after the other on one FPGA.
It can be concluded that a 1500Mp input image using four FPGAs on a network is processed
within three seconds resulting in a total throughput of 500Mp/s.
NFPG As

N columns

i w per FPGA

processing time

throughput

1
2
3
4

2
2
3
4

22.4Kp
22.4Kp
14.9Kp
11.2Kp

12s
6s
4s
3s

62.5Mp/s
250Mp/s
375Mp/s
500Mp/s

Table 6.5: Example scalability across FPGA

63

CHAPTER

7

V ERIFICATION AND B ENCHMARK

With the image processing and dataflow parts implemented, they can be verified and benchmarked. The next chapters hold the verification process of both the image processing part and the
dataflow part. After both components are verified they are benchmarked as one unity against a
computer based implementation in chapter 7.3. Before starting, the following chapter summarizes
all solutions developed in this project.

65

Chapter 7. Verification and Benchmark

7.1

Overview

This chapter lists all the solutions that are compared in verification and benchmark. The solutions
can be found in table 7.1.
Image Processing
HLS 8bit

This solution is a Wallis filter with an 8bit AXI4-Stream as
input and using C/C++ in Vivado HLS and is implemented on
the FPGA.

HLS 256bit

This solution is a Wallis filter with a 256bit AXI4-Stream as
input and using C/C++ in Vivado HLS and is not implemented
on the FPGA.

VHDL 8bit

This solution is a Wallis filter with an 8bit AXI4-Stream as
input and using VHDL and is implemented on the FPGA.

C/C++ Wallis filter

This solution is a Wallis filter using C/C++ on the Computer.

Controller
Controller HLS

The HLS controller reads image data stored in block RAM
using AXI4 interface and sends it to the Wallis filter via AXI4Stream. Works only with memory based communication.

Controller VHDL

The VHDL controller receives image data directly from the
communication part via AXI4-Stream and sends it to the
Wallis filter via AXI4-Stream. Works only with stream based
communication.

Communication
Communication Memory based

Stores the received image data in block RAM using AX4 interface.

Communication
Stream based

Received data is streamed directly to the controller. Data to
send is also transferred using AXI4-Stream.

Overall
diip

This solution includes the HLS 8bit Wallis filter, the HLS
controller and the communication based on memory.

diip_fatser

This solution includes the VHDL 8bit Wallis filter, the VHDL
controller and the communication based on streaming.

Table 7.1: Overview of the solutions for the verification and benchmark

66

7.2. Verification

7.2

Verification

The verification process ensures that all components work as expected. It is split into the image
processing and dataflow parts. They are tested independantly to reduce complexity and simulation
time. The system as a unity is then tested in chapter 7.2.3.

7.2.1

Image Procession

The verification of the image processing is done using two different methods. First, an image is
processed with the Wallis filter and compared with a reference image. The reference image is
generated from a C/C++ program which calculates using floating point precision for calculations.
In the second aspect the throughput is validated.
The devices under validation (DUV) consist of two written in Vivado HLS using C/C++ and one
VHDL version. The Vivado HLS versions differ in the input stream. One has an 8 bit AXI4-Stream
and the other one has a 256 bit AXI4-Stream input. The VHDL has an 8 bit AXI4-Stream.

Image Comparsion
During the verification an image is processed with the Wallis filter. The Wallis filter is run on the
computer as a C/C++ program and has floating point variables. This Wallis filtering is used as a
reference. Two images (room and mountain), added in appendix A.6, are chosen as source images.
The source images are passed through the device under validation (DUV) and compared with the
reference images. The root mean square error (RMSE) serves as a metric. The RMSE indicates
how much the Wallis filtered image deviates on average from the reference image [23]. The
parameters for the Wallis filter that were used for the images can be found in table 7.2.
Table 7.3 lists the RMSE values of the room and mountain image. They represent the deviation
from the reference image in percent. The deviations from the C/C++ program with the 8 bit AXI4
stream and the 256 bit AXI4 stream do not differ from each other. A total deviation of 0.32% has
been measured for the room image. This corresponds to an intensity value of 0.816 in the range
of 0 - 255. There is also only a deviation of 0.33% in the mountain image. The VHDL verification
is with 1.23% respectively 0.49% higher than the HLS versions. The intensity deviates by 3.14
or 1.25 pixel values respectively. It is assumed that the deviation occurs due to rounding in the
VHDL solution.
Image

Brightness

Contrast

Global Mean

Global Variance

room A.1
mountain A.2

0.5
0.5

0.8125
0.8125

127
127

3600
3600

Table 7.2: Parameters for the Wallis filter
67

Chapter 7. Verification and Benchmark

image
HLS 8bit
HLS 256bit
VHDL 8bit

C/C++ Simulation (HLS)
room
mountain

RTL Simulation (HLS)
room
mountain

0.32%
0.32%

0.32%
0.32%

0.33%
0.33%

VHDL Simulation (ghdl)
room
mountain

0.33%
0.33%
1.23%

0.49%

Table 7.3: RMSE of the Wallis filter verification

In comparison to the HLS solution, the VHDL solution does not calculate the mean and the
variance exactly. Further it is assumed that image contents play a role, meaning how strong the
intensity in the image differs in regard to the location. The room image changes intensity more
often, whereby the mountain image is bright at the top and dark at the bottom.
Throughput
During verification, the throughput of the Wallis filter is also evaluated. For this purpose, the
throughput at the input of the filter is measured. This means how many pixels per clock are loaded
into the Wallis filter. Again, the three different methods (HLS 8bit, HLS 256bit and VHDL 8bit)
of the Wallis filter are compared. Table 7.4 lists the three different methods. The 8bit versions
should have a throughput of one pixel per clock, this is 125Mp/s at 125MHz clock frequency. But
the HLS 8 bit version only reaches 21 pixels in 86 clocks. This results in a throughput of 30.5Mp/s.
It is four times slower than the theoretical value. This has to do with the fact that the division
requires 23 clock cycles and that the loop cannot be pipelined by reading the pixels (see chapter
4.2.2 and 4.2.3). For this reason the 256 bit version was developed. It has a theoretical maximum
of 2625Mp/s throughput. A throughput of 21 pixels per 11 clock cycles could be achieved. This is
equivalent to 238.6Mp/s. In relation to the 8 bit version only a better throughput of a factor of
7.8 could be achieved. The limiting factor is still the dataflow in the image processing core of the
pixel reading. With the VHDL version the theoretical maximum of 125Mp/s has been reached.
DUV

Target [p/clk]

Actual [p/clk]

Throughput [Mp/s]

HLS 8bit
HLS 256bit
VHDL 8bit

1
21
1

21/86
21/11
1

30.5
238.6
125

Table 7.4: Throughput of the Wallis filter related to the input pixels at 125MHz clock frequency

68

7.2. Verification

DUV

Slice

(33650)

BRAM

(365)

DSP

(740)

HLS 8bit
HLS 256bit
VHDL 8bit

949
1542
253

(2.8%)
(4.6%)
(0.8%)

10.5
5
1

(2.9%)
(1.4%)
(0.3%)

17
16
16

(2.3%)
(2.2%)
(2.2%)

Table 7.5: Resources of the three implemented Wallis filters

Resource
For the implementation of the Wallis filter, another important factor is the resources it requires
on the FPGA. The resources used on the FPGA for the implementation of the Wallis filter are
listed in table 7.5 as absolute values and as percentage of the available on the FPGA (value in
parentheses). Slices, Block RAMs and DSPs are compared with each other. A slice on the FPGA
(XC7A200T) contains four LUTs and eight flip-flops [24]. The VHDL implementation needs the
least resources in the comparison. All three implementations need practically the same number
of DSPs. Comparing the two HLS versions, the 256bit version needs less block memory, but more
slices.

7.2.2

Dataflow

The dataflow part is again divided into two parts, the communication and controller parts.
While in the communication part only the most recent version is tested (the stream based
communication, referring to chapter 5 explaining the solution A and B with streaming interface),
both versions of the controller are verified.

Communication
The communication part has in a large part been taken from the last semester project and
has been thoroughly tested and validated in the project report [3]. The three new implemented
features are verified in this chapter. They consist of:
1. Acknowledge
2. User registers
3. Stream interface
Acknowledge: To test the acknowledge function, a file was sent to the FPGA from the computer
and the Ethernet traffic was monitored with Wireshark Network Protocol Analyzer. Wiresharks
packet dissections are exported to a json file that was then analyzed using the uftcheck utility.
It was written to analyze network packets for UFT transfers. The first lines of output are shown
in listing 7.1.
69

Chapter 7. Verification and Benchmark

1

+−−−−−−+−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−+−−−−−+−−−−−−−−−−−−+−−−−−−+−−−−−−+

2

| Pack |

3

+−−−−−−+−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−+−−−−−+−−−−−−−−−−−−+−−−−−−+−−−−−−+

From

|

To

| D/C |

Control

4

|

1

| 1 9 2 . 1 6 8 . 5 . 1 0 (50719) | 1 9 2 . 1 6 8 . 5 . 9 (42042)

|

C

|

5

|

2

| 1 9 2 . 1 6 8 . 5 . 1 0 (50719) | 1 9 2 . 1 6 8 . 5 . 9 (42042)

|

D

|

6

|

3

| 1 9 2 . 1 6 8 . 5 . 9 (42042)

| 1 9 2 . 1 6 8 . 5 . 1 0 (50719) |

7

|

5

| 1 9 2 . 1 6 8 . 5 . 1 0 (50719) | 1 9 2 . 1 6 8 . 5 . 9 (42042)

8

|

6

| 1 9 2 . 1 6 8 . 5 . 9 (42042)

FT Start

| TCID | SEQ

|

|

12

| 1036 |

|

12

|

0

|

C

| ACK packet |

12

|

0

|

|

D

|

|

12

|

1

|

| 1 9 2 . 1 6 8 . 5 . 1 0 (50719) |

C

| ACK packet |

12

|

1

|

Listing 7.1: Output of uftcheck for package acknowledgment
The sending PC starts a file transmission with a file transfer start packet and the first data
packet. The next packet (3) is coming from the FPGA to the PC and acknowledges the first data
packet (sequence 0). To verify that all packets are acknowledged, the sending program reports an
acknowledge status after the file has been sent. Listing 7.2 shows the output of the UFT sender
program.
1

$ . / sender 1 9 2 . 1 6 8 . 5 . 9 42042 payload / c a t . jpg

2

UFT Sender demo

3

destination 192.168.5.9:42042

4

HURRAY! A l l 1036 packets have been acknowledged .

5

time elapsed : 1.18 s Speed : 0.859 MB/ s Size : 1.012 MB

Listing 7.2: UFT send console output
User register: To test whether the user registers can be written and are output correctly by
the communication core, an integrated logic analyzer (ILA) was used. An ILA can be configured
on the FPGA to record internal signals. The results are transferred to the PC over USB and
displayed in Vivado HLx. All user registers (0 through 7) are written with different values and
the result was observed using the ILA.
Stream interface: The last modification made to the communication core was the AXI4-Stream
interface. This was similarly tested as the user registers. Using an ILA the output of the stream
was observed and the correct order of data verified.
Controller
The most important thing to verify in the controller core is the correct order of output pixels. This
is done by generating input data representing an image, feeding this data through the controller
core and observing the output pixels. This validation is split into the two solutions implemented
using HLS (solution A) and VHDL (solution B).

70

7.2. Verification

00 01 02 03 04 05 06 07
08 09 0A 0B 0C0D0E 0F
10 11 12 13 14 15 16 17
18 19 1A 1B 1C1D1E 1F
20 21 22 23 24 25 26 27
28 29 2A 2B 2C2D2E 2F
30 31 32 33 34 35 36 37
38 39 3A 3B 3C3D3E 3F

Height

Width
Figure 7.1: VHDL controller validation stimuli, hex values

Solution A) HLS: In Vivado HLS the validation was done in C/C++ and RTL-simulation. A
testbench C/C++ file generates pseudo random image data. This data is then processed using the
controller core and in the testbench itself. The two results are then compared. Using different
input data and image sizes, all output results matched. A test on the FPGA was omitted. This
validation will later take place in the overall validation in chapter 7.2.3.
Solution B) VHDL: The same approach was used in the solution written in VHDL except that
the testbench is not written in C/C++ but in VHDL. The input stream sends an incrementing
pixel value with an image width and height of eight. The window length was reduced to three for
simplicity. Figure 7.1 shows the input data values.
The output stream that was stored in a text file by the VHDL testbench was observed. The
resulting pixel order was then validated using random samples. Listing 7.3 shows the output
pixel values of the first three lines.
1

00 08 10 01 09 11 02 0A 12 03 0B 13 04 0C 14 05 0D 15 06 0E 16 07 0F 17

2

08 10 18 09 11 19 0A 12 1A 0B 13 1B 0C 14 1C 0D 15 1D 0E 16 1E 0F 17 1F

3

10 18 20 11 19 21 12 1A 22 13 1B 23 14 1C 24 15 1D 25 16 1E 26 17 1F 27

Listing 7.3: Output stream hexadecimal coded

7.2.3

Overall Validation

To validate the overall system, the image processing and dataflow parts are combined in a Vivado
HLx project. Two projects are distinguished:
1. diip1 project using solution A) with 8bit HLS implementation
2. diip_faster project using solution B) with 8bit VHDL implementation

1 Distributed Image Processing (diip)

71

Chapter 7. Verification and Benchmark

On the computer side, the program diip_cc written in C++ reads the pixels from an input image,
splits the data into according UFT packets and sends them to the FPGA. The processed pixels are
received and stored in the output image. Similar to the proceeding in 7.2.1 two test images were
used and compared to the results of a floating point calculation on the computer. The resulting
root mean square error (RMSE) is listed in table 7.6. The verification of the entire system on the
FPGA is positive. It can be seen that the HLS solution has the same deviation as the simulation
on the PC (see chapter 7.2.1). The VHDL version has a smaller deviation across the entire system
than the ghdl simulation on the PC (see chapter 7.2.1). It is assumed that the divider model used
in simulation is not as accurate as the implemented version of the divider generator. Also shown
here is that the room image has a larger deviation in the VHDL solution than the mountain
image. This is to be justified with the same reasons as explained in chapter 7.2.1. It is assumed
that this depends on the rounding and the image content.
Solution
diip
diip_faster

RMSE room [%]

RMSE mountain [%]

0.32
0.91

0.33
0.37

Table 7.6: Overall validation results

72

7.3. Benchmark

7.3

Benchmark

To conclude the results of this work, the different solutions are compared in this chapter. The
working implementations (including send and receive from PC) and theoretical limits are differentiated. Furthermore, the throughput is compared to a CPU based solution. There are four
different processing methods in total:
1. The solution using HLS with 8bit data bus
2. The solution using HLS with 256bit data bus. This method was not implemented on the
FPGA hence simulation results were used
3. The solution implemented using VHDL 8bit
4. A program running on CPU written in C++
Methods 1. and 3. are measured by processing an image stored on the computer which is sent to
the FPGA, processed and sent back. The time required for the processing of an image divided by
its pixel count results in the throughput measured in megapixels per second (M p/s, 1p ≡ 1B). To
calculate the throughput of method 2. the results from 7.2.1 are used. Method 4. is written in C++
and uses OpenCV to read image data. Throughput is calculated the same as for methods 1. and 3.
As a source image a fixed aspect ration of 16:9 was chosen. This is a common format used among
others for movies. The image was resized to four different heights: 480p, 720p, 1080p and 2160p
with the largest one being equivalent to the 4k image standard. The content of the image does
not influence the throughput.
The benchmark is split into three separate comparisons. First the two overall working solutions
are compared (diip and diip_faster). Then the theoretical limits of the three image processing
solution are compared without the influence of a controller or the communication part. Finally
the fastest image processing solution is put in comparison with a CPU based processing.

73

Chapter 7. Verification and Benchmark

7.3.1

Compare Implementations

First the two working FPGA implementations are put in contrast (1. and 3.). Figure 7.2 and
table 7.7 show the results of the throughput measurements. The missing value for HLS 8bit is
due to a bug in the computer software for this image size that could not be resolved by the time
the benchmark was made. The HLS 8bit implementation yields the lowest throughput. This is
mainly due to the controller core which requires the image data to be re-sent for every line the
Wallis filter processes (no image lines are cached on the FPGA) meaning that the full potential of
the HLS 8bit image processing core is not taken advantage of. The VHDL implementation yields
the highest throughput. This was to be expected for two reasons. First, the VHDL Wallis filter
core can process pixels at one pixel per clock cycle which is more than the 8bit HLS version is
capeable of (21 pixels per 86 clock cycles). Second, the memory management has been greatly
improved from the HLS to the VHDL controller in that image data is cached on the FPGA to
reduce multiple transmissions of image data from PC to FPGA.

Image Processing Benchmark
HLS 8bit
VHDL

throughput [MB/s]

4
3
2
1
0
500

1,000

1,500

2,000

image height [p]
Figure 7.2: Wallis throughput comparison

Solution

480p

720p

1080p

2160p

HLS 8bit
VHDL 8bit

0.17
0.89

1.291

0.168
2.348

0.161
4.115

Table 7.7: Throughput measurements

74

MB/s
MB/s

7.3. Benchmark

The increase in throughput over image size on the VHDL solution can be explained by the way
the PC program works. To prevent an overflow of the image cache inside the controller on the
FPGA, a delay of d l seconds is inserted after an image line is sent. This ensures that the FPGA is
given enough time to calculate one image line. From there the theoretical maximum throughput
can be calculated. Equation 7.1 shows the throughput (for derivation refer to appendix A.7.1). d l
was set to 500µ s for the benchmark.

b≈

b=

iw
d l + bi we

(7.1)

theoretical throughput of VHDL solution

iw =

image width

dl =

delay between sending two image lines

be =

ethernet throughput

If d l were to be reduced to zero, the throughput would theoretically strive towards b e . Although
as explained in the next chapter, the maximum throughput is limited by the Wallis filter core.

7.3.2

Compare Theoretical Limits

To show the real performance of an FPGA for image processing the theoretical limits are put
in contrast in this chapter without considering how the image data is sent to the FPGA. Before
this can be done the maximum possible throughput of the three FPGA implementations are
calculated.

bip =

bip =
ih =

i h bw
wl (i h − wl + 1)

(7.2)

theoretical throughput image processing core
image height

bw =

throughput of image processing core on input in pixels per second

wl =

window length

Equation 7.2 is used to calculate the maximum possible throughput for each of the test images
(for derivation refer to appendix A.7.2). Figure 7.3 and table 7.8 show the theoretical maximum
results.
75

Chapter 7. Verification and Benchmark

The HLS 8bit solution is the slowest of the three implementations. Its filter core accepts 21 pixels
per 86 clock cycles as described in table 7.4. With the input width of the IP core extended to
265bit, a new throughput of around 11 MB/s is achieved. The third solution written in VHDL is
placed in between the two HLS solutions. Its IP core accepts one pixel per clock cycle. This yields
a throughput of approximately 5.8MB/s at 125MHz clock frequency.
Image Processing Theoretical Maximums

throughput [MB/s]

12

HLS 8bit
HLS 256bit
VHDL

10
8
6
4
2
0

500

1,000

1,500

2,000

image height [p]
Figure 7.3: Theoretical maximum throughput of FPGA

Solution

bw

480p

720p

1080p

2160p

HLS 8bit
HLS 256bit
VHDL 8bit

29.1
228
119

1.45
11.3
5.92

1.43
11.1
5.84

1.41
11.0
5.78

1.40
10.9
5.73

MB/s
MB/s
MB/s

Table 7.8: FPGA implementations theoretical limits

76

7.3. Benchmark

7.3.3

FPGA against CPU

Now that the different implementations are compared against each other, the two implementations with the highest throughput (VHDL and HLS 256bit implementations) are put in contrast
with a CPU based computation. The CPU program was run multiple times for each image and
the mean throughput was calculated. The program was running on an Intel Core i7-6700HQ
running at 2.60 GHz on a single core. Figure 7.4 and table 7.9 show the results.

The CPU performance is the fastest of all three. It processes the images 2.76 times faster than
the VHDL solution and 1.45 times faster than the HLS 256bit solution. As mentioned in the
scalability chapter (6) the throughput of both FPGA implementations (VHDL and HLS 256bit)
can be improved by implementing the Wallis filter multiple times on one FPGA. By implementing
the HLS solution twice or the VHDL solution three times, a regular CPU could already be
outrunned by one FPGA.

Theoretical Maximum vs CPU
HLS 256bit
VHDL
CPU

throughput [MB/s]

16
14
12
10
8
6
4
2
0

500

1,000

1,500

2,000

image height [p]
Figure 7.4: Theoretical maximum throughput of FPGA versus CPU

Solution

480p

720p

1080p

2160p

VHDL 8bit
HLS 256bit
CPU

5.92
11.3
12.56

5.84
11.1
16.23

5.78
11.0
15.99

5.73
10.9
15.91

MB/s
MB/s
MB/s

Table 7.9: FPGA PC Throughput comparison

77

Chapter 7. Verification and Benchmark

7.3.4

Throughput against Area Efficiency

Finally, the throughput efficiency of the Wallis filter is examined. The three Wallis filter implementations are compared. The efficiency of the implementations is examined depending
on the area they use on the FPGA and their theoretical throughput. The area is shown in
table 7.5. Only one resource is considered for meaningful efficiency. The DSP is omitted for
comparison, since all three implementations consume the same amount. The block memory is
also not selected, because the controller needs much more BRAM than the Wallis filter and it
needs less memory than slices in percentage. For this reason, the slice usage is compared for
throughput efficiency. Table 7.10 lists the comparison. The efficiency is measured in kB/s per slice.
The VHDL code yields the best efficiency by far. This can be explained in two reasons: One, the
VHDL code was written on register transfer level, meaning that every register and combinatorial
logic was placed deliberate while the HLS synthesis tool optimizes C/C++ code to a certain degree
but not perfect. This can be compared to writing microcontroller code in either C or assembly
language. The second reason being that the dataflow inside the core could be described more
precise in VHDL leading to a higher throughput.
The difference in the two HLS implementations can be explained by the way the 256bit differs
from the 8bit version. The increase in input width resulted only in a minor increase in resource
usage. But it enabled the whole processing to be pipelined which requires little more resources
but yields high throughput increase. The strongly increased throughput lead to an efficiency
increase of a factor of five.

DUV

Slice usage

Throughput [MB/s]

Efficiency [kB/s per slice]

HLS 8bit
HLS 256bit
VHDL 8bit

969
1542
253

1.4
11.0
5.8

1.5
7.3
23.5

Table 7.10: Efficiency of the Wallis filter in relation to area and throughput

78

CHAPTER

8

C ONCLUSION

In this last chapter the project results are briefly summarized and a short outlook for possible
future work is given.

8.1

Image Processing

A new image processing core was implemented that locally optimizes contrast (Wallis filter).
Multiple implementations have been validated and compared. Using high level synthesis to
describe an algorithm has proven that the desired operation can be achieved in little time.
Nevertheless the theoretical possible throughput could not be achieved due to the dataflow not
being described well enough in C/C++ language to achieve the desired throughput. Therefore a
VHDL implementation was written that processes pixels at 125Mp/s and uses approximately
three percent of FPGA resources.

8.2

Dataflow

The existing communication core was extended with acknowledge, user registers and AXI4Stream interfaces. The new image processing algorithm requires the input data in a specific order.
Therefore a controller was implemented using two different design flows. The HLS approach
showed that complex interfaces such as AXI4 can be implemented with a few lines of code and
that state machines can be implemented as well. The unfavourable memory management and
the lack of high throughput led to the implementation of a VHDL based controller and memory
management unit. It caches necessary image data to prevent multiple image transmissions and
can support the full throughput of the image processing core.
79

Chapter 8. Conclusion

8.3

Complete System

The final product called Distributed Image Processing (diip) is based on the VHDL implementations of image processing and dataflow parts. Images can be sent from the PC to the FPGA
where the Wallis operation is applied and the image is sent back. Image data throughput of up
to 4.1MB/s have been measured. The system is designed with scalability in mind. A dedicated
theoretical examination shows that if the image processing core was implemented 21 times on
the FPGA the full bandwidth of gigabit Ethernet could be used to process image data yielding
125MB/s throughput. Furthermore, if multiple FPGAs were used, the total throughput would
scale proportional to the number of FPGAs used.

8.4

Working with High Level Synthesis

The time spent on working with Vivado HLS has shown several advantages and disadvantages.
It has shown that thinking close to hardware is crucial. Using C/C++ as language is a trap to fall
back and think of the code as if it would be executed sequentially. The function to be implemented
should be split into building blocks the same way as it would be done using a hardware description
language. If the code was written with the exact same line of thought as a HDL approach, then
the same throughput should be achievable. And if that is managed, HLS would bring significant
advantages like the simple implementation of complex interfaces (e.g. AXI4) and the reduced
time in testbench development.

8.5

Future Work

The scalability is proven on paper and can now be implemented onto the FPGA. The controller
core can be extended to send the data to multiple Wallis filter cores. After writing a computer
application that can handle the fast transfer speeds the true benefit of using FPGA for image
processing can be shown in praxis. Because the data transfer is Ethernet based a cluster of
FPGAs can be built to compete against high performance CPU and GPU based image processing
pipelines. Another method to increase throughput would be to implement a VHDL 256bit image
processing core that would be able to process 21 pixels per clock cycle and produce one pixel per
clock cycle on the output. This would omit the scalability inside the FPGA because the Ethernet
link would already be saturated. Furthermore, the HLS implementations could be improved by
describing the dataflow more hardware oriented and prove that using HLS the same throughput
can be achieved as when using a hardware description language.

80

G LOSSARY

BRAM tile
A single block ram instance on the FPGA. In this case one tile can be configured as either
one 36Kb RAM or two inpdependant 18Kb RAM
device under validation (DUV)
An entity that is tested inside a testbench.
Distributed Image Processing (diip)
Project nickname of the bachelor thesis
FiFo
First-in First-out databuffer. Data written is read in the same order. Data can only be read
once.
ghdl
Open-source simulator used for simulating VHDL entities
integrated logic analyzer (ILA)
Can be configured to record FPGA internal signals and send to PC
IP core
Intellectual Property core. Functional blocks that are connected together and form the
FPGA configuration. IP cores can be bought or developed.
OpenCV
Open-source image processing library used in the C/C++ PC software.
root mean square error (RMSE)
Used as a metric to measure the difference in two images
81

Glossary

RTL
Register transfer level. Design abstraction which models a synchronous digital circuit in
terms of dataflow between hardware registers.
Slice
A slice on the FPGA (XC7A200T) contains four LUTs and eight flip-flops [24].
UDP File Transfer (UFT)
The session protocol introduced in the preceeding project implemented on the FPGA used
to transfer image data to and from the FPGA.
Vivado HLS
Xilinx tool used for high level synthesis. Hardware is described in C/C++ language that is
synthesized by the tool to hardware description language.
Vivado HLx
Xilinx tool used for synthesis, implementation and debugging

82

B IBLIOGRAPHY

[1]

V. F. Brena-Medina, University of bristol thesis template. [Online]. Available: https://

www.overleaf.com/latex/templates/university- of- bristol- thesis- template/
kzqrfvyxxcdm#.W3PNyJMzbUI (visited on Aug. 15, 2018).
[2]

Nomoko, Nomoko website, Aug. 2018. [Online]. Available: http://dev.nomoko.world/
(visited on Aug. 15, 2018).

[3]

N. Huetter and J. Stocker, Distributed FPGA for enhanced imaged processing - 5th semester
project, FHNW, Jan. 2018.

[4]

Brockhaus, “Bildverarbeitung (Informatik)”, in Die Brockhaus Enzyklopädie Online, 2018. [Online]. Available: https : / / brockhaus . de / ecs / permalink /

DDF1D3EAE8DC41F6B17CF7016841075E.pdf (visited on Aug. 15, 2018).
[5]

G. N. Chaple, R. D. Daruwala, and M. S. Gofane, “Comparisions of robert, prewitt, sobel
operator based edge detection methods for real time uses on FPGA”, in 2015 International
Conference on Technologies for Sustainable Development (ICTSD), Feb. 2015, pp. 1–4.

DOI:

10.1109/ICTSD.2015.7095920.
[6]

L.-w. Wang and A. Wang, “Virtual texture with wallis filter for terrain visualization”, in
Engineering Science and Technology Review, Jul. 2013.

[7]

L. Zeng, Mail: P6 - update [16], Apr. 2018.

[8]

Ipv4 header, 2018. [Online]. Available: https://nmap.org/book/images/hdr/MJB-IP-

Header-800x576.png (visited on Jun. 13, 2018).
[9]

Vivado design suite user guide, 2014. [Online]. Available: https://www.xilinx.com/

support / documentation / sw _ manuals / xilinx2014 _ 1 / ug902 - vivado - high - level synthesis.pdf (visited on Jul. 26, 2018).
[10]

Hls pragmas, 2018. [Online]. Available: https : / / www . xilinx . com / html _ docs /

xilinx2017_4/sdsoc_doc/okr1504034364623.html (visited on Jul. 26, 2018).
[11]

Reduce power and cost by converting from floating point to fixed point, 2017. [Online].
Available: https://www.xilinx.com/support/documentation/white_papers/wp491-

floating-to-fixed-point.pdf (visited on Jul. 31, 2018).

83

Bibliography

[12]

High-level synthesis, 2014. [Online]. Available: https : / / www . xilinx . com / support /

documentation/sw_manuals/xilinx2014_1/ug902- vivado- high- level- synthesis.
pdf (visited on Jul. 25, 2018).
[13]

Amba AXI and ACE protocol specification, 2011. [Online]. Available: http://www.gstitt.

ece.ufl.edu/courses/fall15/eel4720_5721/labs/refs/AXI4_specification.pdf
(visited on Jul. 30, 2018).
[14]

AMBA 4 AXI4-stream protocol, 2010. [Online]. Available: http://www.mrc.uidaho.edu/

mrc/people/jff/EO_440/Handouts/AMBA%20Protocols/AXI-Stream/IHI0051A_amba4_
axi4_stream_v1_0_protocol_spec.pdf (visited on Aug. 2, 2018).
[15]

nomoko, Distributed FPGA for enhanced imaged processing, STP0005, nomoko, Mar. 2017.

[16]

AC701 picture, 2018. [Online]. Available: https://www.xilinx.com/content/xilinx/

en / products / boards - and - kits / ek - a7 - ac701 - g / _jcr _ content / mainParsys /
xilinxtabs2/tab- hardware/xilinximage.img.jpg/1504134323246.jpg (visited on
Jun. 13, 2018).
[17]

Xilinx AC701 evaulation kit, 2018. [Online]. Available: https : / / www . xilinx . com /

products / boards - and - kits / ek - a7 - ac701 - g . html # hardware (visited on Jun. 13,
2018).
[18]

Axi4-stream infrastructure IP suite v2.2, 2018. [Online]. Available: https://www.xilinx.

com/support/documentation/ip_documentation/axis_infrastructure_ip_suite/
v1_1/pg085-axi4stream-infrastructure.pdf (visited on Jul. 30, 2018).
[19]

Divider generator v5.1, 2016. [Online]. Available: https://www.xilinx.com/support/

documentation/ip_documentation/div_gen/v5_1/pg151- div- gen.pdf (visited on
Aug. 2, 2018).
[20]

Opencv basic structures, 2018. [Online]. Available: https://docs.opencv.org/2.4/

modules/core/doc/basic_structures.html (visited on Jul. 31, 2018).
[21]

Designing protocol processing systems with vivado high-level synthesis, 2018. [Online].
Available: https://www.xilinx.com/support/documentation/application_notes/

xapp1209 - designing - protocol - processing - systems - hls . pdf (visited on Jul. 31,
2018).
[22]

Gs418tpp gigabit PoE+ smart managed pro switches, 2018. [Online]. Available: https:

//www.netgear.com/business/products/switches/smart/GS418TPP.aspx (visited on
Aug. 6, 2018).
[23]

D. Fenna, Root-mean-square error, Jan. 2004. [Online]. Available: http : / / www .

oxfordreference . com / view / 10 . 1093 / acref / 9780198605225 . 001 . 0001 / acref 9780198605225-e-1223 (visited on Aug. 15, 2018).

84

[24]

7 series FPGAs data sheet: Overview, 2018. [Online]. Available: https://www.xilinx.

com/support/documentation/data_sheets/ds180_7Series_Overview.pdf (visited on
Aug. 9, 2018).

L IST OF F IGURES

2.1

Border problem with neighborhood filter operations . . . . . . . . . . . . . . . . . . . . .

6

2.2

Image processing operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

2.3

Wallis filtered image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.4

OSI Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.5

Ethernet Frame . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.6

IPv4 header [8] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.7

Vivado HLS design flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

2.8

Pragma: interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

2.9

Pragma: array_partition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

2.10 Pragma: pipeline / unroll . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

2.11 VALID before READY handshake [13] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

3.1

Xilinx AC701 Evaluation Kit [16] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

3.2

Block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

4.1

Concept of the Wallis filter implementation . . . . . . . . . . . . . . . . . . . . . . . . . .

26

4.2

Sequence of the implementation with initialization and iteration . . . . . . . . . . . . .

26

4.3

Concept of the mean and variance implementation in VHDL . . . . . . . . . . . . . . .

32

4.4

Simulation of the dir_shift_reg and the sum_diff block with a WIN_SIZE of 9 . . .

32

4.5

Concept of the Wallis filter implementation in VHDL . . . . . . . . . . . . . . . . . . . .

33

4.6

Data width of mean and variance calculation . . . . . . . . . . . . . . . . . . . . . . . . .

34

5.1

Dataflow inside FPGA for solution A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

5.2

Dataflow inside FPGA for solution B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

5.3

UFT Top Block Design with acknowledge path . . . . . . . . . . . . . . . . . . . . . . . .

38

5.4

UFT core with AXI4-Lite interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

5.5

UFT Top Block Design with AXI4-Stream interface . . . . . . . . . . . . . . . . . . . . .

41

5.6

UFT IP core with AXI4-Stream interface . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

5.7

Memory reordering problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

85

5.8

Scenario A) Data growth in horizontal direction . . . . . . . . . . . . . . . . . . . . . . .

44

5.9

Scenario B) Data growth in vertical direction . . . . . . . . . . . . . . . . . . . . . . . . .

44

5.10 Controller solution A) simplified state machine . . . . . . . . . . . . . . . . . . . . . . .

46

5.11 Memory layout for solution A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

47

5.12 Controller solution B) block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

5.13 FiFo structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

51

5.14 Memory layout for solution B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

5.15 Solution B output data stream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54

6.1

56

Block diagram of a inside FPGA scaled solution . . . . . . . . . . . . . . . . . . . . . . .
0

6.2

Comparing proposals with variable image width (wl = N = 21, i h = 10 000) . . . . . . .

60

6.3

Image partitioning for distribution across three FPGAs . . . . . . . . . . . . . . . . . .

61

6.4

Network topology required to distribute workload onto multiple FPGAa . . . . . . . .

62

7.1

VHDL controller validation stimuli, hex values . . . . . . . . . . . . . . . . . . . . . . .

71

7.2

Wallis throughput comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

74

7.3

Theoretical maximum throughput of FPGA . . . . . . . . . . . . . . . . . . . . . . . . . .

76

7.4

Theoretical maximum throughput of FPGA versus CPU . . . . . . . . . . . . . . . . . .

77

A.1 Source image room for the image processing verification . . . . . . . . . . . . . . . . . . 110
A.2 Source image mountain for the image processing verification . . . . . . . . . . . . . . . 110

L IST OF TABLES

2.1

TCP vs. UDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

2.2

AXI4 transaction channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

2.3

AXI4-Stream required signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

2.4

AXI4-Stream sideband signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

3.1

Xilinx AC701 key board features [17] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

3.2

XC7A200T key features [17] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

4.1

Constants of the code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

4.2

Accuracy of data types in the Wallis filter division . . . . . . . . . . . . . . . . . . . . . .

29

4.3

Comparision of division methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

86

List of Tables

5.1

UFT command list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

5.2

ACK signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

5.3

UFT core register map. RO=read only, WO=write only . . . . . . . . . . . . . . . . . . .

40

5.4

Controller solution A interface ports . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

5.5

Controller solution B interface ports (unlisted interfaces are not used) . . . . . . . . .

49

6.1

Parameters used for scalability proposal . . . . . . . . . . . . . . . . . . . . . . . . . . .

55

6.2

Inside FPGA scalability proposals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

58

6.3

FPGA block memory budget . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

60

6.4

Parameter summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

60

6.5

Example scalability across FPGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

7.1

Overview of the solutions for the verification and benchmark . . . . . . . . . . . . . . .

66

7.2

Parameters for the Wallis filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

7.3

RMSE of the Wallis filter verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

68

7.4

Throughput of the Wallis filter related to the input pixels at 125MHz clock frequency

68

7.5

Resources of the three implemented Wallis filters . . . . . . . . . . . . . . . . . . . . . .

69

7.6

Overall validation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72

7.7

Throughput measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

74

7.8

FPGA implementations theoretical limits . . . . . . . . . . . . . . . . . . . . . . . . . . .

76

7.9

FPGA PC Throughput comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77

7.10 Efficiency of the Wallis filter in relation to area and throughput . . . . . . . . . . . . .

78

A.1 Description of data types in the Wallis filter division . . . . . . . . . . . . . . . . . . . . 102

87

APPENDIX

A

A PPENDICES

A.1

Aufgabenstellung 2018 P6 Distributed FPGA

89

P6 Aufgabenstellung für
Herr Noah Hütter
Herr Jan Stocker

Distributed FPGA for enhanced Image Processing
1. Ausgangslage
Die Firma Nomoko AG ist ein Spin-off der ETH Zürich. Ihre Vision ist, mit einer hochauflösenden
Kamera ein digitales Abbild der Realität zu erzeugen. Hierzu entwickeln sie eine 1500 Megapixel
Kamera!
Im Projekt 5 haben Sie sich in die Thematik eingearbeitet. Es resultierte ein Framework, welches
Daten zwischen Host und FPGA austauschen kann und ein Core, in welchem ein Algorithmus
mittels Vivado HLS implementiert werden kann.

2. Aufgabenstellung
Entwickeln Sie das Framework aus Projekt 5 weiter. Die genauen Zielsetzungen an das Framework
sollen am 28. Februar 2018 gemeinsam mit Nomoko definiert werden. Machen Sie Vorschläge, in
welche Richtungen es gehen könnte (Bildverarbeitung: Welche Algorithmen sollen implementiert
werden / Skalierung: Framework mit N FPGAs (Konzept, Theorie, Praxis) / Kommunikation: Wie sind
die Anforderungen an das UFT). Erstellen Sie anschliessend eine Projektklärung, in welcher Sie
aufzeigen, wie Sie die gesteckten Ziele erreichen wollen.

3. Organisation
Auftraggeber:
Experte:
Betreuer:
Arbeitsort:
Meetings:

Nomoko AG
Dr. Jürg Stettbacher
Michael Pichler
4.223 (IME Labor, Steinackerstr. 5, Windisch)
nach Bedarf

4. Form des Resultats
•
•
•
•

Abgabe einer Planung / Projektklärung
Schriftliche Dokumentation in Papierform und Design-Daten in elektronischer Form
Mündliche Präsentation / Verteidigung vor dem Auftraggeber, den Dozierenden und weiteren
Interessenten.
Weitere Deliverables gemäss den Anforderungen des Studiengangs

5. Termine
Ausgabe der Arbeit:
Abgabe der Projektklärung:
Abgabe der Dokumentation:
Präsentation:

19. Februar 2018
09. März 2018
31. August 2018, 12:00 Uhr
voraussichtlich 14. September 2018

Windisch, den 14. Februar 2018
Michael Pichler
Institut für Mikroelektronik (IME)

Steinackerstrasse 5
5210 Windisch

T +41 56 202 80 22
F +41 56 462 73 17

info.ime.technik@fhnw.ch
www.fhnw.ch/ime

1

Technical Note - STP0005
March 2017 - Revised May 2017

Distributed FPGA for
enhanced Imaged Processing
Keywords
FPGA, LAN, Computer Vision, Digital Circuit Design

Abstract
In order to accelerate the intense image processing tasks we want to investigate a distributed dedicated hardware approach. To do so, basic computer
vision algorithms such as Edge-Detection should be implemented on a Network consisting out of 4 FPGA boards such as the ARTIX7. To keep it simple,
the network should be built up over Ethernet or any preferable other protocol.
The FPGAs should then sharing the workload and contribute individually to
the greater good.

Contents
Task Description
Task 1: Research and Feasibility Check (20 %) . . . . . . . . . . .
Task 2: Investigate and Evaluate Computer Vision Algorithms (30 %)
Task 3: Implement the distributed FPGA Algorithm (40 %) . . . . .
Task 4: Present your achievement (10 %) . . . . . . . . . . . . . .

1
2
2
2
2

Milestones

2

Requirements

3

Correspondence

3

Task Description
The student is asked to evaluate a distributed FPGA system for the purpose of
image processing. A hardware module has to be implemented that performs
a basic computer vision task in a distributed matter. Once this is achieved the
system should either be extended to perform a more sophisticated task or to
more than 4 FPGAs. During the Project, the student is expected to perform the
following tasks. These task do not necessarily have to be done in the following
order.
Distributed FPGA for enhanced Imaged Processing
1
Copyright © 2017 nomoko AG

STP0005 - March 2017

Task 1: Research and Feasibility Check (20 %)

Task 1: Research and Feasibility Check (20 %)
• Set up the FPGA network and check project feasibility.
• Find related research considering distributed FPGA designs.

Task 2: Investigate and Evaluate Computer Vision Algorithms
(30 %)
• What algorithms can easily be implemented on an FPGA or a system of
FPGAs - try!.
• What part of the algorithms can be distributed.
• Evaluate and take a decision with respect to consumed area, timing issues
and own defined metrics. (e.g. AT-product, Throughput, etc.).

Task 3: Implement the distributed FPGA Algorithm (40 %)
• Implement the chosen algorithm on the 4 FPGAs.
• Compare different use cases with 1, 2, 3 and for FPGAs involved.
• Push the system by either implementing a more complex algorithm or
extend it to more FPGAs, how does it scale?

Task 4: Present your achievement (10 %)
• Write a report, document your project and comment your code.
• Prepare a presentation!

Milestones
The goal of the project is to evaluate the benefits of a distributed FPGA system.
We want to know if it makes sense to rely on such a system and integrate it into
our network to perform certain tasks. Over the project the following milestones
are expected to be reached:
• Successfully set up the FPGA network.
• Find a suitable computer vision algorithm to be implemented on the network.
• Implement the algorithm on the network and compare performance in
between different use cases.
• Perform an outstanding presentation.
Distributed FPGA for enhanced Imaged Processing
2
Copyright © 2017 nomoko AG

STP0005 - March 2017

Requirements
Students that are interested in hardware design and networks. Generally students that are able to work independently and want to explore new technical
challenges.
• VHDL/Verilog
• Computer Vision
• Ethernet Protocol

Correspondence
Engineer:
Email:

Matteo Pavan, Patrick Oschatz
matteo(at)nomoko.camera, patrick(at)nomoko.camera

Distributed FPGA for enhanced Imaged Processing
3
Copyright © 2017 nomoko AG

STP0005 - March 2017

Appendix A. Appendices

A.2

Technicial Requirements

94

Distributed FPGA for Enhanced Image Processing
Technical Requirements

Noah Hütter, Jan Stocker
Windisch, 21.02.2018

IME – Institute of Microelectronics

Steinackerstrasse 5
5210 Windisch

noah.huetter@students.fhnw.ch
jan.stocker@students.fhnw.ch

Table of contents
1

Dates

3

2

Contact

3

3

Work Environment

3

4

Documentation

3

4.1

Organizational Requirements Sheet

3

4.2

Technical Documentation

3

4.3

Presentation

3

5

Goals

4

5.1

First Semester

4

5.2

Second Semester (Bachelor Thesis)

4

6

Technical specifications

5

6.1

Data Transmission

5

6.2

Data Format

5

6.3

Data Size

5

6.4

Image Processing Algorithm

5

6.5

Distributed Processing

6

6.5.1

Inside FPGA

6

6.5.2

Across FPGA

6

6.6

Platform

6

6.7

Design Entry

7

6.7.1

Image Processing

7

6.7.2

Communications

7

Page 2/7

1

Dates

Project start
Submission of planning
Submission of documentation
Presentation

2

19.02.2018
09.03.2018
31.08.2018
14.09.2018

Contact

Noah Hütter
Student
noah.huetter@students.fhnw.ch
T +41 76 562 07 19

Jan Stocker
Student
jan.stocker@students.fhnw.ch
T +41 79 890 71 23

Michael Pichler
Coach
michael.pichler@fhwn.ch
T +41 56 202 75 26

Nomoko
matteo@nomoko.ch
quentin@nomoko.ch
kevin@nomoko.ch
T +41 44 309 81 11

3

Work Environment

The team Jan Stocker and Noah Hütter are working at FHNW Brugg-Windisch. They are doing
a weekly digest by Mail to Nomoko.

4

Documentation

4.1

Organizational Requirements Sheet

A Rough time plan is created showing the milestones and the 4 tasks:
1. Research and feasibility check (20%)
2. Investigate and evaluate computer vision algorithms (30%)
3. Implement the distributed FPGA algorithm (40%)
4. Present you achievement (10%)
4.2

Technical Documentation

A technical report is written showing the problems and solutions. The code is commented. One
documentation is sufficient for both companies.
4.3

Presentation

The team will perform a presentation about the results achieved at FHNW and Nomoko.

Page 3/7

technical_requirements.docx

5

Goals

The project is divided into two parts.
5.1

First Semester

By the 12/01/2018 a working platform is implemented with the following features:
• Data transmission from PC to FPGA and back
• Basic image processing task performed by the FPGA
• Keep the possibility open to distribute the workload to multiple FPGAs
5.2

Second Semester (Bachelor Thesis)
•
•
•

Page 4/7

Distribute the workload onto multiple FPGAs
Implement a more demanding image processing task
Benchmark the performance against a CPU/GPU

technical_requirements.docx

6

Technical specifications

6.1

Data Transmission

The data is transmitted over Ethernet to the FPGA system. The outcome of project 5 (the UDP
file transfer protocol) serves as a base and is further improved in the following aspects:
•
•
•

Implementation of reliable transfer
Make use of the entire gigabit LAN connection
Write a computer side cross-platform implementation to transfer data

Furthermore, the subsequent modifications are investigated on their impact on performance:
•
•
6.2

The use of jumbo frames (packet size larger than 1600 bytes)
Heartbeat from the FPGA to the computer instead of packet acknowledgment
Data Format

The images have the data format of tiff and for the raw images dng.
6.3

Data Size

The input images are in the order of 1500M Pixel.
6.4

Image Processing Algorithm

The following image processing algorithms are considered:
• Debayering
• Color space transform
• Local contrast enhancement
• Edge detection
Furthermore, multiple different algorithms may be implemented to perform multiple tasks with or
without reconfiguring the FPGA:
• Multiple algorithms on the FPGA, select through Ethernet
• Different bit streams with different algorithms, partial reconfigure by the FPGA itself
6.5

Benchmark

The implemented algorithm(s) are compared with a solution based on CPU/GPU in the following
criteria:
• Throughput
• Latency
• Image quality
• Maximum clock frequency
The communication part is also benchmarked in terms of:
• Throughput
• Latency
• Packet loss

Page 5/7

technical_requirements.docx

6.6

Distributed Processing

6.6.1

Inside FPGA

Data transmission inside the FPGA must be defined. Possible solutions are FiFo, Block RAM
and Streaming. This depends on the image processing algorithm and what image data it requires. The following approaches are examined and compared.
Block RAM
Every image processing core is connected to block memory. A control block distributes the incoming data among the cores and handles the data flow.

Streaming
The connection inside the FPGA is over a stream interface. The data are distributed to the image processing core by a AXI-Stream interconnect.

6.6.2

Across FPGA

Develop a PC application that takes the image data and handles the communication with the
FPGAs. A minimal console application serves the purpose. Cross-platform compatibility
(Mac/Windows/Linux) is necessary. The FPGAs don’t communicate among themselves.
6.7

Platform

The platform used is a Xilinx AC-701 board with Artix 7 series FPGA provided by Nomoko. It
features:
• DDR3 SODIMM 1GB up to 533MHz / 1066Mbps
• 10/100/1000 Mbps Ethernet (RGMII)
• Onboard JTAG configuration circuitry to enable configuration over USB

Page 6/8

technical_requirements.docx

6.8

Design Entry

6.8.1

Image Processing

The image processing algorithm is programmed in C/C++ and then compiled to HDL using the
Vivado HLS environment. VHDL is used as hardware description language.
Develop a deep understanding on the Vivado HLS workflow.
6.8.2

Communications

The communication is implemented using configurable IP blocks provided by Xilinx or others.
VHDL is used as hardware description language.

Page 7/7

technical_requirements.docx

Appendix A. Appendices

A.3

Data Types of Wallis Filter

Name

Description

iPxl
g_Mean
g_Var
n_Mean
n_Var
brightness
contrast
b_gMean
ci_gVar
tmp_Num
num
c_nVar
bi_nMean
den_Var
rec
den
div
w_pixel

Pixel from the input image which is calculated
Global mean of the output image
Global variance of the output image
Mean of the neighborhood
Variance of the neighborhood
Brightness factor
Contrast factor
Multiplication of brightness and global mean
Multiplication of (1 - contrast) and global variance
Temporary numerator of the division without contrast
Numerator of the division with contrast
Multiplication of contrast and neighborhood variance
Multiplication of (1 - brigthness) and neighborhood mean
Denumerator of the division
Variable has value 1 to calculate a reciprocal division
Result of the reciprocal division
Result of the entire Wallis filter division
Output pixel from the Wallis filter
Table A.1: Description of data types in the Wallis filter division

102

A.4. UDP File Transfer (UFT) Protocol Specification

A.4

UDP File Transfer (UFT) Protocol Specification

103

UDP File Transfer (UFT) Protocol Specification

Noah Huetter
Windisch, 14.08.2018

IME – Institute of Microelectronics

Steinackerstrasse 5
5210 Windisch

T +41 56 202 75 29

noah.huetter@students.fhnw.ch
jan.stocker@students.fhnw.ch

1

Ethernet frame

Maximum data payload
IP header
UDP Header
Data

1’500 Bytes
-20 bytes
-8 Bytes
1’472 Bytes

D/C bit
Transaction ID size
Sequence number size
Remaining data size

-1 Bit
-31 Bit
-4 Bytes
1'464 Bytes

Max file size
N data packets

4GB = 4’096MB = 4'194'304KB = 4'294'967'296 Bytes
2'933'721

D/C bit
Transaction ID size
Sequence number size
Remaining data size

-1 Bit
-31 Bit
-4 Bytes
1'464 Bytes

Max file size
N data packets

4GB = 4’096MB = 4'194'304KB = 4'294'967'296 Bytes
2'933'721

Page 2/4

UDP File Transfer.docx

2

Packet types

2.1

Control Packet

0
1
2
3
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
0 0 Command [6:0]
Command Data 1 [23:0]
4 Command Data 2 [31:0]
8 Padding 26 Bytes
2.2

Data Packet
0
1
2
3
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7

0 1 Transaction ID [6:0]
4

Page 3/4

Packet sequence number [23:0]
Data

UDP File Transfer.docx

3

Command Packet

3.1

Command list

Code
0x00
0x01
0x02
0x03
0x04

3.2

Short
FTS
FTP
ACKFP
ACKFT
USER

0x03
Transaction ID
0x0000 0000

USER: User register

Command field:
Data 1:
Data 2:

Page 4/4

0x02
Transaction ID
Acknowledged sequence number

ACKFT: Acknowledge file transfer

Command field:
Data 1:
Data 2:
3.6

0x01
Transaction ID
0x0000 0000

ACKFP: Acknowledge data packet

Command field:
Data 1:
Data 2:
3.5

0x00
Transaction ID
Number of sequences to be transmitted

FTP: File transfer stop

Command field:
Data 1:
Data 2:
3.4

Data 1
TCID
TCID
TCID
TCID
REG

FTS: File transfer start

Command field:
Data 1:
Data 2:
3.3

Name
File transfer start
File transfer stop
Acknowledge data packet
Acknowledge file transfer
Set user registers in receiver

0x04
User register address
User register data

UDP File Transfer.docx

Data 2
NSEQ
0x0000 0000
SEQNBR
0x0000 0000
data

Appendix A. Appendices

A.5

UDP File Transfer Calculation

108

UFT Data calculation
Filesize
Packet tx delay

4 GB
100 us
GB

MB

Maximum file size to be sent
Time between two packets
KB

Maximum data payload
IP header
UDP Header
Data

B
1500
-20
-8
1472

b
12000
-160
-64
11776

-4
1468

-1
-7
-24
11744

Method 1: Max data payload
D/C bit
Transaction ID size
Sequence number size
Remaining data size
File Size
N data packets

No packet delay

4 4096 4194304 4294967296 34359738368
2925728
estimated Tx time

34.36
119.21
326.93
12.53

estimated datarate

With packet delay

Tx time
datarate

s
MB/s
s
MB/s

Method 2: 1024 bytes payload

1024

Remaining data size
File Size
N data packets

No packet delay

4 4096 4194304 4294967296 34359738368
4194304
Tx time
datarate

With packet delay

8192

Tx time
datarate

34.36
119.21
453.79
9.03

s
MB/s
s
MB/s

Appendix A. Appendices

A.6

Images for Wallis Filter

Figure A.1: Source image room for the image processing verification

Figure A.2: Source image mountain for the image processing verification
110

A.7. Derivations

A.7
A.7.1

Derivations
Theoretical maximum Throughput of VHDL Solution
b=
=
=
=
=
≈

b=

ip

(A.1)

tt
iw ih
t i + (i h − wl + 1)t r
iw ih

(A.3)

( bi we + d l )wl + (i h − wl + 1)( bi we + d l )
iw ih
( bi we

(A.4)

+ d l )(i h + 1)

iw ih

(A.5)

d l (i h + 1) + bi we (i h + 1)
iw

(A.6)

d l + bi we

theoretical throughput of VHDL solution

ip =

total image pixels

tt =

total processing time

iw =

image width

ih =

image height

ti =

time to send the initial lines

wl =

(A.2)

windows length

tr =

iterration time to process one line

be =

ethernet throughput

dl =

delay between sending two image lines

Throughput b is calculated by dividing the total amount of data i p by the time required t t to
process the data. This total time can be split into a single initialization time t i and a multiple of
the iteration time t r . The initialization time is only required once and the iteration is executed
i h − wl + 1 times (this is the image width subtracted by the window lendth, refer to border
handling problem). For initialization, wl lines are sent with a transfer time of
One iteration takes

iw
be

iw
be

+ d l seconds.

+ d l seconds (time for one line). Factoring out i h + 1 and assuming i w À 1

the final equation can be noted.

111

Appendix A. Appendices

A.7.2

Theoretical maximum Throughput of VHDL Wallis Core
b=
=

ip

(A.7)

tw
iw ih

(A.8)

n ps
bw

i w i h bw
wl i w (i h − wl + 1)
i h bw
=
wl (i h − wl + 1)
bw
=
wl (1 − wi hl + i1h )

(A.9)

=

≈

b=

(A.11)

bw
wl

(i h À wl )

(A.12)

theoretical throughput of VHDL Wallis filter

ip =

total image pixels

tw =

total processing time of Wallis filter

iw =

image width

ih =

image height

n ps =

(A.10)

Number of pixels processed by Wallis filter on input

bw =

throughput of Wallis filter

wl =

window length

Throughput b is calculated by dividing the total amount of data i p by the time required t w to
process the data. The total time to compute the data is equivalent to the total amount of pixels
n ps being processed divided by the input throughput b w of the Wallis filter core. The total amount
of pixels is the number of output lines (i h − wl + 1, border handling problem) times the input line
size wl · i w (wl lines are required for the neighborhood operation). Assuming i h À wl , the final
equation can be noted.

112

